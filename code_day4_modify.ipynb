{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_day4_ALL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ehi1150I7fxN",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hH-OAGszyef5",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
        "fluff, id = link_hilstrom.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Hillstrom.csv')\n",
        "hillstrom_df = pd.read_csv('Hillstrom.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7tAS92JMPe9U",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "link_lalonde = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
        "fluff, id = link_lalonde.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Lalonde.csv')\n",
        "lalonde_df = pd.read_csv('Lalonde.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyJ1oeuTMFhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link_criteo_fix = 'https://drive.google.com/open?id=13pNFAeH1ZEPxaCU9tQz1H17L28aTcppg'\n",
        "fluff, id = link_criteo_fix.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('criteo_small_fix.csv')\n",
        "criteo_df = pd.read_csv('criteo_small_fix.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZoNrZI5P80wJ",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "\n",
        "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
        "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
        "    #   as the target variable of interest and the selected treatment is \n",
        "    #   the e-mail campaign for women’s merchandise [1]\n",
        "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
        "    #    J Market Anal. 2014;2:218–238\n",
        "    dataset = dataset.lower()\n",
        "    if dataset in ('hillstrom', 'email'):\n",
        "        columns = df.columns\n",
        "        for col in columns:\n",
        "            if df[col].dtype != object:\n",
        "                continue\n",
        "            df = pd.concat(\n",
        "                    [df, pd.get_dummies(df[col], \n",
        "                                        prefix=col, \n",
        "                                        drop_first=False)],\n",
        "                    axis=1)\n",
        "            df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
        "                      for col in df.columns]\n",
        "        df = df[df.segment_mens_email == 0]\n",
        "        df.index = range(len(df))\n",
        "        df.drop(['segment_mens_email', \n",
        "                 'segment_no_email', \n",
        "                 'conversion', \n",
        "                 'spend'], axis=1, inplace=True)\n",
        "\n",
        "        y_name = 'visit'\n",
        "        t_name = 'segment_womens_email'\n",
        "    elif dataset in ['criteo', 'ad']:\n",
        "        df = df.fillna(0)\n",
        "        y_name = 'y'\n",
        "        t_name = 'treatment'\n",
        "    elif dataset == 'lalonde':\n",
        "        y_name = 'RE78'\n",
        "        t_name = 'treatment'\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    df['Y'] = df[y_name]\n",
        "    df.drop([y_name], axis=1, inplace=True)\n",
        "    df['T'] = df[t_name]\n",
        "    df.drop([t_name], axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Igf3QLgdJ1cW",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def performance(pr_y1_t1, pr_y1_t0, y, t, groups=10):\n",
        "    \"\"\"\n",
        "    1. Split the total customers into the given number of groups\n",
        "    2. Calculate the statistics of each segment\n",
        "    \n",
        "    Args:\n",
        "        pr_y1_t1: the series (list) of the customer's expected return\n",
        "        pr_y1_t0: the expected return when a customer is not treated\n",
        "        y: the observed return of customers\n",
        "        t: whther each customer is treated or not\n",
        "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
        "    Return:\n",
        "        DataFrame:\n",
        "            columns:\n",
        "                'n_y1_t1': the number of treated responders\n",
        "                'n_y1_t0': the number of not treated responders\n",
        "                'r_y1_t1': the average return of treated customers\n",
        "                'r_y1_t0': the average return of not treated customers\n",
        "                'n_t1': the number of treated customers\n",
        "                'n_t0': the number of not treated customers\n",
        "                'uplift': the average uplift (the average treatment effect)\n",
        "            rows: the index of groups\n",
        "    \"\"\"\n",
        "  \n",
        "    ### check valid arguments\n",
        "    if groups not in [5, 10, 20]:\n",
        "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
        "  \n",
        "    ### check for NAs.\n",
        "    if pr_y1_t1.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in pr_y1_t1\")\n",
        "    if pr_y1_t0.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in pr_y1_t0\")\n",
        "    if y.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in y\")\n",
        "    if t.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in t\")\n",
        "   \n",
        "    ### check valid values for y and t\n",
        "    # if set(y) != {0, 1}:\n",
        "    #     raise Exception(\"uplift: y must be either 0 or 1\")\n",
        "    if set(t) != {0, 1}:\n",
        "        raise Exception(\"uplift: t must be either 0 or 1\")\n",
        "\n",
        "    ### check length of arguments\n",
        "    if not (len(pr_y1_t1) == len(pr_y1_t0) == len(y) == len(t)):\n",
        "        raise Exception(\"uplift: arguments pr_y1_t1, pr_y1_t0, y and t must all have the same length\")\n",
        "\n",
        "    ### define dif_pred\n",
        "    dif_pred = pr_y1_t1 - pr_y1_t0\n",
        "  \n",
        "    ### Make index same\n",
        "    y.index = dif_pred.index\n",
        "    t.index = dif_pred.index\n",
        "    \n",
        "    mm = pd.DataFrame({\n",
        "        'dif_pred': dif_pred,\n",
        "        'y': y,\n",
        "        't': t,\n",
        "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
        "    })\n",
        "\n",
        "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
        "  \n",
        "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
        "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
        "    n_t1 = mm_groupby['t'].sum()\n",
        "    n_t0 = mm_groupby['t'].count() - n_t1\n",
        "  \n",
        "    df = pd.DataFrame({\n",
        "        'n_t1': n_t1,\n",
        "        'n_t0': n_t0,\n",
        "        'n_y1_t1': n_y1_t1,\n",
        "        'n_y1_t0': n_y1_t0,\n",
        "        'r_y1_t1': n_y1_t1 / n_t1,\n",
        "        'r_y1_t0': n_y1_t0 / n_t0,\n",
        "    })\n",
        "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
        "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
        "    df.index.name = 'groups'\n",
        "\n",
        "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
        "    df['uplift'] = round(df['uplift'], 6)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def qini(perf, plotit=True):\n",
        "    nrow = len(perf)\n",
        "\n",
        "    # Calculating the incremental gains. \n",
        "    # - First, the cumulitative sum of the treated and the control groups are\n",
        "    #  calculated with respect to the total population in each group at the\n",
        "    #  specified decile\n",
        "    # - Afterwards we calculate the percentage of the total amount of people\n",
        "    #  (both treatment and control) are present in each decile\n",
        "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
        "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
        "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
        "\n",
        "    ### Model Incremental gains\n",
        "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
        "    inc_gains = [0.0] + list(inc_gains)\n",
        "\n",
        "    ### Overall incremental gains\n",
        "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
        "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
        "\n",
        "    ### Random incremental gains\n",
        "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
        "\n",
        "    ### Compute area under the model incremental gains (uplift) curve\n",
        "    x = [0] + deciles\n",
        "    y = list(inc_gains)\n",
        "    auuc = 0\n",
        "    auuc_rand = 0\n",
        "\n",
        "    auuc_list = [auuc]\n",
        "    for i in range(1, len(x)):\n",
        "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
        "        auuc_list.append(auuc)\n",
        "\n",
        "    ### Compute area under the random incremental gains curve\n",
        "    y_rand = random_inc_gains\n",
        "\n",
        "    auuc_rand_list = [auuc_rand]\n",
        "    for i in range(1, len(x)):\n",
        "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
        "        auuc_rand_list.append(auuc_rand)\n",
        "\n",
        "    ### Compute the difference between the areas (Qini coefficient)\n",
        "    Qini = auuc - auuc_rand\n",
        "\n",
        "    ### Plot incremental gains curve\n",
        "    if plotit:\n",
        "        x_axis = x\n",
        "        plt.plot(x_axis, inc_gains)\n",
        "        plt.plot(x_axis, random_inc_gains)\n",
        "        plt.show()\n",
        "    \n",
        "    ### Qini 30%, Qini 10%\n",
        "    n_30p = int(nrow*3/10)\n",
        "    n_10p = int(nrow/10)\n",
        "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
        "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
        "\n",
        "    res = {\n",
        "        'qini': Qini,\n",
        "        'inc_gains': inc_gains,\n",
        "        'random_inc_gains': random_inc_gains,\n",
        "        'auuc_list': auuc_list,\n",
        "        'auuc_rand_list': auuc_rand_list,\n",
        "        'qini_30p': qini_30p,\n",
        "        'qini_10p': qini_10p,\n",
        "    }    \n",
        "\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nrxjl1v7J9Mm",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "def parameter_tuning(fit_mdl, pred_mdl, data, search_space, plotit=False):\n",
        "    \"\"\"\n",
        "    Given a model, search all combination of parameter sets and find\n",
        "    the best parameter set\n",
        "    \n",
        "    Args:\n",
        "        fit_mdl: model function\n",
        "        pred_mdl: predict function of fit_mdl\n",
        "        data:\n",
        "            {\n",
        "                \"x_train\": predictor variables of training dataset,\n",
        "                \"y_train\": target variables of training dataset,\n",
        "                \"t_train\": treatment variables of training dataset,\n",
        "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
        "                \"y_test\": target variables of test (usually, validation) dataset,\n",
        "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
        "            }\n",
        "        search_space:\n",
        "            {\n",
        "                parameter_name: [search values]\n",
        "            }\n",
        "    Return:\n",
        "        The best parameter set\n",
        "    \"\"\"\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['y_train']\n",
        "    t_train = data['t_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "    t_test = data['t_test']\n",
        "    \n",
        "    max_q = -float('inf')\n",
        "    best_mdl = None\n",
        "\n",
        "    keys = search_space.keys()\n",
        "    n_space = [len(search_space[key]) for key in keys]\n",
        "    n_iter = np.prod(n_space)\n",
        "    \n",
        "    best_params = None\n",
        "    for i in range(n_iter):\n",
        "        params = {}\n",
        "        for idx, key in enumerate(keys):\n",
        "            params[key] = search_space[key][i % n_space[idx]]\n",
        "            i = int(i / n_space[idx])\n",
        "\n",
        "        mdl = fit_mdl(x_train, y_train, t_train, **params)\n",
        "        pred = pred_mdl(mdl, newdata=x_test, y=y_test, t=t_test, ct=t_test)\n",
        "        # print('    {}'.format(params))\n",
        "        try:\n",
        "            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "        q = qini(perf, plotit=plotit)['qini']\n",
        "        if plotit:\n",
        "            print(q, params)\n",
        "        if q > max_q:\n",
        "            max_q = q\n",
        "            best_mdl = mdl\n",
        "            best_params = params\n",
        "\n",
        "    return best_mdl, best_params\n",
        "\n",
        "\n",
        "def wrapper(fit_mdl, pred_mdl, data, params=None,\n",
        "            best_models=None, drop_variables=None, qini_values=None, wrapper_variables=None):\n",
        "    \"\"\"\n",
        "    General wrapper approach\n",
        "    \n",
        "    Args:\n",
        "        fit_mdl: model function\n",
        "        pred_mdl: predict function of fit_mdl\n",
        "        data:\n",
        "            {\n",
        "                \"x_train\": predictor variables of training dataset,\n",
        "                \"y_train\": target variables of training dataset,\n",
        "                \"t_train\": treatment variables of training dataset,\n",
        "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
        "                \"y_test\": target variables of test (usually, validation) dataset,\n",
        "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
        "            }\n",
        "    Return:\n",
        "        (A list of best models, The list of dropped variables)\n",
        "    \"\"\"\n",
        "    if best_models is None:\n",
        "        best_models = []\n",
        "    if drop_variables is None:\n",
        "        drop_variables = []\n",
        "    if qini_values is None:\n",
        "        qini_values = []\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['y_train']\n",
        "    t_train = data['t_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "    t_test = data['t_test']\n",
        "\n",
        "    variables = data['x_train'].columns\n",
        "\n",
        "    max_q = -float('inf')\n",
        "    drop_var = None\n",
        "    best_mdl = None\n",
        "    for var in variables:\n",
        "        if var in drop_variables:\n",
        "            continue\n",
        "        x = x_train.copy()\n",
        "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
        "        mdl = fit_mdl(x, y_train, t_train, **params)\n",
        "        x = x_test.copy()\n",
        "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
        "        pred = pred_mdl(mdl, newdata=x, y=y_test, t=t_test, ct=t_test)\n",
        "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
        "        q = qini(perf, plotit=False)['qini']\n",
        "        if q > max_q:\n",
        "            max_q = q\n",
        "            drop_var = var\n",
        "            best_mdl = mdl\n",
        "    \n",
        "#         previous_max = wrapper_variables.loc[len(drop_variables) + 1, fit_mdl.__name__]\n",
        "#         if q > previous_max:\n",
        "#             wrapper_variables.loc[len(drop_variables) + 1, fit_mdl.__name__] = q\n",
        "    \n",
        "    \n",
        "    best_models.append(best_mdl)\n",
        "    drop_variables.append(drop_var)\n",
        "    qini_values.append(max_q)\n",
        "\n",
        "    left_vars = [var for var in variables if (var not in drop_variables)]\n",
        "    \n",
        "    if len(variables) == len(drop_variables) + 1:\n",
        "        return best_models, drop_variables + left_vars, qini_values\n",
        "    else:\n",
        "        return wrapper(fit_mdl, pred_mdl, data, params=params,\n",
        "                       best_models=best_models, drop_variables=drop_variables,\n",
        "                       qini_values=qini_values, wrapper_variables=wrapper_variables)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxrEbEODlbQi",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def tma(x, y, t, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
        "    (a.k.a. \"Separate Model Approach\")\n",
        "    The default model is General Linear Model (GLM)\n",
        "    \n",
        "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        Dictionary: A dictionary of two models. One for the treatment group, \n",
        "            one for the control group.\n",
        "\n",
        "            {\n",
        "                'model_treat': a model for the treatment group,\n",
        "                'model_control': a model for the control group\n",
        "            }\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    treat_rows = (t == 1)\n",
        "    control_rows = (t == 0)\n",
        "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
        "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
        "    \n",
        "    res = {\n",
        "        'model_treat': model_treat,\n",
        "        'model_control': model_control,\n",
        "    }\n",
        "    return res\n",
        "\n",
        "\n",
        "def predict_tma(obj, newdata, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
        "    (a.k.a. \"Separate Model Approach\")\n",
        "    \n",
        "    For each instance in newdata two predictions are made:\n",
        "    1) What is the probability of a person responding when treated?\n",
        "    2) What is the probability of a person responding when not treated\n",
        "      (i.e. part of control group)?\n",
        "\n",
        "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
        "\n",
        "    Args:\n",
        "        obj: A dictionary of two models. \n",
        "            One for the treatment group, one for the control group.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        DataFrame: A dataframe with predicted returns for when the customers\n",
        "            are treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "   \n",
        "    if isinstance(obj['model_treat'], LinearRegression):\n",
        "        pred_treat = obj['model_treat'].predict(newdata)\n",
        "    else:\n",
        "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
        "\n",
        "    if isinstance(obj['model_control'], LinearRegression):\n",
        "        pred_control = obj['model_control'].predict(newdata)\n",
        "    else:\n",
        "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
        "    \n",
        "    # pred_treat = obj['model_treat'].predict(newdata)\n",
        "    # pred_control = obj['model_control'].predict(newdata)\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hREl_CRv9DYC",
        "outputId": "ea25fc59-5210-4b52-c7eb-4c2e395ebdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def ty_assign(y, t):\n",
        "    if y == 1 and t == 1:\n",
        "        return \"TR\"\n",
        "    elif y == 0 and t == 1:\n",
        "        return \"TN\"\n",
        "    elif y == 1 and t == 0:\n",
        "        return \"CR\"\n",
        "    elif y == 0 and t == 0:\n",
        "        return \"CN\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def t_assign(ty):\n",
        "    if ty in (\"TR\", \"TN\"):\n",
        "        return 1\n",
        "    elif ty in (\"CR\", \"CN\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def y_assign(ty):\n",
        "    if ty in (\"TR\", \"CR\"):\n",
        "        return 1\n",
        "    elif ty in (\"TN\", \"CN\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "search_space = {\n",
        "    'method': [LogisticRegression],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
        "    'penalty': ['none', 'l2'],\n",
        "    'tol': [1e-2, 1e-3, 1e-4],\n",
        "    'C': [1e6, 1e3, 1, 1e-3, 1e-6],\n",
        "}\n",
        "\n",
        "def main():\n",
        "    ### Load data ###\n",
        "#     datasets = [hillstrom_df, lalonde_df, criteo_df]\n",
        "    datasets = [hillstrom_df]\n",
        "#     dataset_type = ['hillstrom', 'lalonde', 'criteo']\n",
        "    dataset_type = ['hillstrom']\n",
        "    target_models = {\n",
        "#         'dta': [(dta, predict_dta)],\n",
        "#         'tma': [(tma, predict_tma)],\n",
        "#         'trans': [(lai, predict_lai), (glai, predict_glai), (rvtu, predict_rvtu)],\n",
        "        'tree': [(uplift_tree, predict_tree), (upliftRF, predict_upliftRF)],\n",
        "#         'trans': [(rvtu, predict_rvtu)]\n",
        "    }\n",
        "#     df = pd.read_csv('Hillstrom.csv')\n",
        "#     dataset = 'hillstrom'\n",
        "\n",
        "#     fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "    \n",
        "    for i, dataset in enumerate(datasets):\n",
        "        df = dataset\n",
        "        df = preprocess_data(df, dataset=dataset_type[i])\n",
        "        Y = df['Y']\n",
        "        T = df['T']\n",
        "        X = df.drop(['Y', 'T'], axis=1)\n",
        "        ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
        "               .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)\n",
        "        if dataset_type[i] == 'hillstrom':\n",
        "            fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=3126).split(X, ty)\n",
        "        else:\n",
        "            fold_gen = KFold(n_splits=5, shuffle=True, random_state=3126).split(X)\n",
        "        \n",
        "        wrapper_variables = pd.DataFrame(index = range(len(X.columns)))\n",
        "        \n",
        "        qini_resultset = {}\n",
        "        ### Cross validation ###        \n",
        "        for idx, (train_index, test_index) in enumerate(fold_gen):\n",
        "            qini_list = pd.DataFrame(index = ['qini', 'inc_gains', 'random_inc_gains','auuc_list','auuc_rand_list','qini_30p','qini_10p'])\n",
        "            X_train = X.reindex(train_index)\n",
        "            X_test = X.reindex(test_index)\n",
        "            Y_train = Y.reindex(train_index)\n",
        "            Y_test = Y.reindex(test_index)\n",
        "            T_train = T.reindex(train_index)\n",
        "            T_test = T.reindex(test_index)\n",
        "\n",
        "            df = X_train.copy()\n",
        "            df['Y'] = Y_train\n",
        "            df['T'] = T_train\n",
        "            stratify = T_train\n",
        "            if dataset_type[i] == 'hillstrom':\n",
        "                stratify = df[['Y', 'T']]\n",
        "            tuning_df, validate_df = train_test_split(\n",
        "              df, test_size=0.33, random_state=3126, stratify=stratify)\n",
        "\n",
        "            X_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
        "            Y_tuning = tuning_df['Y']\n",
        "            T_tuning = tuning_df['T']\n",
        "\n",
        "            X_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
        "            Y_validate = validate_df['Y']\n",
        "            T_validate = validate_df['T']\n",
        "\n",
        "            data_dict = {\n",
        "              \"x_train\": X_tuning,\n",
        "              \"y_train\": Y_tuning,\n",
        "              \"t_train\": T_tuning,\n",
        "              \"x_test\": X_validate,\n",
        "              \"y_test\": Y_validate,\n",
        "              \"t_test\": T_validate,\n",
        "            }\n",
        "\n",
        "            for target_model in target_models:\n",
        "                print(target_model)\n",
        "                for model, predict in target_models[target_model]:\n",
        "                    print(model.__name__)\n",
        "#                     if model.__name__ not in wrapper_variables.columns:\n",
        "#                         wrapper_variables[model.__name__] = -float('inf')\n",
        "                    model_method = search_space.get('method', None)\n",
        "                    params = {\n",
        "                      'method': None if model_method is None else model_method[0],\n",
        "                    }\n",
        "                    if params['method'] == LogisticRegression:\n",
        "                        if target_model == 'trans':\n",
        "                            solver = 'liblinear'\n",
        "                        solver = search_space.get('solver', None)\n",
        "                        params['solver'] = None if solver is None else solver[0]\n",
        "\n",
        "                    print(\"Start warpper \" + str(idx))\n",
        "                    if target_model == 'tree':\n",
        "                        for method in ['KL', 'ED', 'Chisq', 'Int']:\n",
        "                            search_space_tree = {\n",
        "                                'method': [method],\n",
        "                                'max_depth': [5, ],\n",
        "                                'min_split': [1000],\n",
        "                                'min_bucket_t0': [100,],\n",
        "                                'min_bucket_t1': [100,],\n",
        "                            }\n",
        "                            if model.__name__ == 'upliftRF':\n",
        "                                search_space_tree['ntree'] = [10]\n",
        "                                search_space_tree['mtry'] = [3]\n",
        "                                search_space_tree['bagging_fraction']: [0.6]\n",
        "                            _, best_params = parameter_tuning(model, predict, data_dict, \n",
        "                                                        search_space=search_space_tree)\n",
        "                            mdl = model(X_train, Y_train, T_train, **best_params)\n",
        "                            pred = predict(mdl, X_test, y=Y_test, t=T_test)\n",
        "                            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
        "                            q = qini(perf)\n",
        "                            key = model.__name__ + '-' + method\n",
        "                            if key not in qini_resultset.keys():\n",
        "                                qini_resultset[key] = []\n",
        "                            qini_resultset[key].append(pd.DataFrame(q))\n",
        "                            print(qini_resultset[key])\n",
        "                    elif target_model != 'tree':\n",
        "                        _, drop_vars, qini_values = wrapper(\n",
        "                              model, predict, data_dict, params=params)\n",
        "                        best_qini = max(qini_values)\n",
        "                        best_idx = qini_values.index(best_qini)\n",
        "                        best_drop_vars = drop_vars[:best_idx]\n",
        "\n",
        "                        X_tuning.drop(best_drop_vars, axis=1, inplace=True)\n",
        "                        X_validate.drop(best_drop_vars, axis=1, inplace=True)\n",
        "                        X_train.drop(best_drop_vars, axis=1, inplace=True)\n",
        "                        X_test.drop(best_drop_vars, axis=1, inplace=True)\n",
        "\n",
        "                        _, best_params = parameter_tuning(model, predict, data_dict, \n",
        "                                                        search_space=search_space)\n",
        "                        mdl = model(X_train, Y_train, T_train, **best_params)\n",
        "                        pred = predict(mdl, X_test, y=Y_test, t=T_test)\n",
        "                        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
        "                        \n",
        "                        q = qini(perf)\n",
        "                        if model.__name__ not in qini_resultset.keys():\n",
        "                            qini_resultset[model.__name__] = []\n",
        "                        qini_resultset[model.__name__].append(pd.DataFrame(q))\n",
        "                        print(qini_resultset[model.__name__])\n",
        "\n",
        "                    \n",
        "#                         print(pd.DataFrame(q).head())\n",
        "#                         print(pd.DataFrame(q.values()))\n",
        "#                         qini_list[model.__name__] = pd.DataFrame(q.values())\n",
        "#         wrapper_variables.plot()\n",
        "#         plt.title(\"Variable Selection - Dataset \" + str(idx) + \": \" + dataset_type[i])\n",
        "#         plt.xlabel(\"Amount of Variables\")\n",
        "#         plt.ylabel(\"Qini Value\")\n",
        "        #         plt.legend(bbox_to_anchor=(1.05, 1), loc=(i+1) * 2 - 1 , borderaxespad=0.)\n",
        "            print(qini_resultset.keys())\n",
        "#         print('Qini values: ', qini_list)\n",
        "#         print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n",
        "#         plt.show()\n",
        "\n",
        "#         plt.subplot(141)\n",
        "#         plt.plot([1,2,3], label=\"test1\")\n",
        "#         plt.plot([3,2,1], label=\"test2\")\n",
        "#         # Place a legend to the right of this smaller subplot.\n",
        "#         plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "         \n",
        "\n",
        "main()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree\n",
            "uplift_tree\n",
            "Start warpper 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "        qini  inc_gains  random_inc_gains  ...  auuc_rand_list  qini_30p  qini_10p\n",
            "0   0.004269   0.000000          0.000000  ...        0.000000  0.000825  0.000098\n",
            "1   0.004269   0.006472          0.004518  ...        0.000226  0.000825  0.000098\n",
            "2   0.004269   0.013373          0.009037  ...        0.000904  0.000825  0.000098\n",
            "3   0.004269   0.017468          0.013555  ...        0.002033  0.000825  0.000098\n",
            "4   0.004269   0.027424          0.018074  ...        0.003615  0.000825  0.000098\n",
            "5   0.004269   0.030034          0.022592  ...        0.005648  0.000825  0.000098\n",
            "6   0.004269   0.031936          0.027111  ...        0.008133  0.000825  0.000098\n",
            "7   0.004269   0.033949          0.031629  ...        0.011070  0.000825  0.000098\n",
            "8   0.004269   0.041867          0.036148  ...        0.014459  0.000825  0.000098\n",
            "9   0.004269   0.043498          0.040666  ...        0.018300  0.000825  0.000098\n",
            "10  0.004269   0.045185          0.045185  ...        0.022592  0.000825  0.000098\n",
            "\n",
            "[11 rows x 7 columns]\n",
            "[        qini  inc_gains  random_inc_gains  ...  auuc_rand_list  qini_30p  qini_10p\n",
            "0   0.004269   0.000000          0.000000  ...        0.000000  0.000825  0.000098\n",
            "1   0.004269   0.006472          0.004518  ...        0.000226  0.000825  0.000098\n",
            "2   0.004269   0.013373          0.009037  ...        0.000904  0.000825  0.000098\n",
            "3   0.004269   0.017468          0.013555  ...        0.002033  0.000825  0.000098\n",
            "4   0.004269   0.027424          0.018074  ...        0.003615  0.000825  0.000098\n",
            "5   0.004269   0.030034          0.022592  ...        0.005648  0.000825  0.000098\n",
            "6   0.004269   0.031936          0.027111  ...        0.008133  0.000825  0.000098\n",
            "7   0.004269   0.033949          0.031629  ...        0.011070  0.000825  0.000098\n",
            "8   0.004269   0.041867          0.036148  ...        0.014459  0.000825  0.000098\n",
            "9   0.004269   0.043498          0.040666  ...        0.018300  0.000825  0.000098\n",
            "10  0.004269   0.045185          0.045185  ...        0.022592  0.000825  0.000098\n",
            "\n",
            "[11 rows x 7 columns]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9IUQwo38PxnJ9fUEUjpSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "        qini  inc_gains  random_inc_gains  ...  auuc_rand_list  qini_30p  qini_10p\n",
            "0   0.004151   0.000000          0.000000  ...        0.000000  0.000673  0.000128\n",
            "1   0.004151   0.007079          0.004518  ...        0.000226  0.000673  0.000128\n",
            "2   0.004151   0.010136          0.009037  ...        0.000904  0.000673  0.000128\n",
            "3   0.004151   0.019687          0.013555  ...        0.002033  0.000673  0.000128\n",
            "4   0.004151   0.026924          0.018074  ...        0.003615  0.000673  0.000128\n",
            "5   0.004151   0.034798          0.022592  ...        0.005648  0.000673  0.000128\n",
            "6   0.004151   0.033969          0.027111  ...        0.008133  0.000673  0.000128\n",
            "7   0.004151   0.032583          0.031629  ...        0.011070  0.000673  0.000128\n",
            "8   0.004151   0.038627          0.036148  ...        0.014459  0.000673  0.000128\n",
            "9   0.004151   0.041041          0.040666  ...        0.018300  0.000673  0.000128\n",
            "10  0.004151   0.045185          0.045185  ...        0.022592  0.000673  0.000128\n",
            "\n",
            "[11 rows x 7 columns]\n",
            "[        qini  inc_gains  random_inc_gains  ...  auuc_rand_list  qini_30p  qini_10p\n",
            "0   0.004151   0.000000          0.000000  ...        0.000000  0.000673  0.000128\n",
            "1   0.004151   0.007079          0.004518  ...        0.000226  0.000673  0.000128\n",
            "2   0.004151   0.010136          0.009037  ...        0.000904  0.000673  0.000128\n",
            "3   0.004151   0.019687          0.013555  ...        0.002033  0.000673  0.000128\n",
            "4   0.004151   0.026924          0.018074  ...        0.003615  0.000673  0.000128\n",
            "5   0.004151   0.034798          0.022592  ...        0.005648  0.000673  0.000128\n",
            "6   0.004151   0.033969          0.027111  ...        0.008133  0.000673  0.000128\n",
            "7   0.004151   0.032583          0.031629  ...        0.011070  0.000673  0.000128\n",
            "8   0.004151   0.038627          0.036148  ...        0.014459  0.000673  0.000128\n",
            "9   0.004151   0.041041          0.040666  ...        0.018300  0.000673  0.000128\n",
            "10  0.004151   0.045185          0.045185  ...        0.022592  0.000673  0.000128\n",
            "\n",
            "[11 rows x 7 columns]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njUjoCyGEG5HQF0IIN/L/3OPRo47vaXMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-4699d1c995de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-4699d1c995de>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                                 \u001b[0mqini_resultset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                             \u001b[0mqini_resultset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqini_resultset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# from BlockManager perspective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             val = sanitize_array(val, index, dtype=dtype, copy=False,\n\u001b[0;32m--> 277\u001b[0;31m                                  raise_cast_failure=False)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mhomogenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;31m# scalar like, GH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, take_fast_path, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_extension_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             subarr = construct_1d_ndarray_preserving_na(subarr, dtype,\n\u001b[0;32m--> 706\u001b[0;31m                                                         copy=copy)\n\u001b[0m\u001b[1;32m    707\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_ndarray_preserving_na\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \"\"\"\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"U\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9X42GUaj-UX",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class Node(object):\n",
        "    def __init__(self, attribute, threshold):\n",
        "        self.attr = attribute\n",
        "        self.thres = threshold\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.leaf = False\n",
        "        self.predict = None, None\n",
        "\n",
        "\n",
        "def info_gain(df, attribute, predict_attr, treatment_attr,\n",
        "              method, min_bucket_t0, min_bucket_t1):\n",
        "    \"\"\"\n",
        "    Select the information gain and threshold of the attribute to split\n",
        "    The threshold chosen splits the test data such that information gain is maximized\n",
        "    \n",
        "    Return a pandas.DataFrame\n",
        "        columns: 'thres' (threshold) and 'info_gain' (information gain)\n",
        "    \"\"\"\n",
        "    num_total = df.shape[0]\n",
        "    tmp = pd.DataFrame({\n",
        "        'thres': df[attribute],\n",
        "        'Y': df[predict_attr],\n",
        "        'T': df[treatment_attr]\n",
        "    })\n",
        "    tmp.sort_values(['thres'], inplace=True)\n",
        "\n",
        "    tmp['n_t1_L'] = (tmp['T']).cumsum()\n",
        "    tmp['n_t0_L'] = (tmp['T'] == 0).cumsum()\n",
        "    tmp['n_t1_R'] = sum(tmp['T']) - (tmp['T']).cumsum()\n",
        "    tmp['n_t0_R'] = sum(tmp['T'] == 0) - (tmp['T'] == 0).cumsum()\n",
        "    tmp['n_y_t1_L'] = (tmp['T'] & tmp['Y']).cumsum()\n",
        "    tmp['n_y_t0_L'] = ((tmp['T'] == 0) & tmp['Y']).cumsum()\n",
        "    tmp['n_y_t1_R'] = sum(tmp['T'] & tmp['Y']) - (tmp['T'] & tmp['Y']).cumsum()\n",
        "    tmp['n_y_t0_R'] = sum((tmp['T'] == 0) & tmp['Y']) - ((tmp['T'] == 0) & tmp['Y']).cumsum()\n",
        "        \n",
        "    # min bucket condition\n",
        "    #   Check the size of treatment & control group in left & right child\n",
        "    tmp['min_bucket_ok'] = ((tmp['n_t1_L'] >= min_bucket_t1) & \n",
        "                            (tmp['n_t0_L'] >= min_bucket_t0) &\n",
        "                            (tmp['n_t1_R'] >= min_bucket_t1) &\n",
        "                            (tmp['n_t0_R'] >= min_bucket_t0))\n",
        "    \n",
        "    if sum(tmp['min_bucket_ok']) > 0:\n",
        "        num_total = df.shape[0]\n",
        "        tr, tn, cr, cn = num_class(df, predict_attr, treatment_attr)\n",
        "        n_t1 = tr + tn\n",
        "        n_t0 = cr + cn\n",
        "        pr_t1 = (tr + tn) / (num_total)\n",
        "        # r_t0 = (tr + cr) / (num_total)\n",
        "        pr_t0 = 1 - pr_t1\n",
        "        pr_y1_t1 = tr / (tr + tn)\n",
        "        pr_y1_t0 = cr / (cr + cn)\n",
        "\n",
        "        # Randomized assignment implies pr_l_t1 = pr_l_t0 for all possible splits\n",
        "        pr_l_t1 = (tmp['n_t1_L']) / (n_t1)\n",
        "        pr_l_t0 = (tmp['n_t0_L']) / (n_t0)\n",
        "        pr_l = pr_l_t1 * pr_t1 + pr_l_t0 * pr_t0\n",
        "        pr_r = 1 - pr_l\n",
        "\n",
        "        # Add Laplace correction to probablities\n",
        "        pr_y1_l_t1 = (tmp['n_y_t1_L']) / (tmp['n_t1_L'])\n",
        "        pr_y1_l_t0 = (tmp['n_y_t0_L']) / (tmp['n_t0_L'])\n",
        "        pr_y1_r_t1 = (tmp['n_y_t1_R']) / (tmp['n_t1_R'])\n",
        "        pr_y1_r_t0 = (tmp['n_y_t0_R']) / (tmp['n_t0_R'])\n",
        "\n",
        "        # Number of treatment/control observations at left and right child nodes\n",
        "        n_t1_L = tmp['n_t1_L']\n",
        "        n_t0_L = tmp['n_t0_L']\n",
        "        n_t1_R = tmp['n_t1_R']\n",
        "        n_t0_R = tmp['n_t0_R']\n",
        "\n",
        "        if method.lower() == 'ed':\n",
        "            tmp['info_gain'] = eucli_dist(tmp,\n",
        "                                          pr_y1_t1,\n",
        "                                          pr_y1_t0,\n",
        "                                          pr_l,\n",
        "                                          pr_r,\n",
        "                                          pr_y1_l_t1,\n",
        "                                          pr_y1_l_t0,\n",
        "                                          pr_y1_r_t1,\n",
        "                                          pr_y1_r_t0,\n",
        "                                          pr_t1,\n",
        "                                          pr_t0,\n",
        "                                          pr_l_t1,\n",
        "                                          pr_l_t0)\n",
        "        elif method.lower() == 'kl':\n",
        "            tmp['info_gain'] = kl_divergence(tmp,\n",
        "                                            pr_y1_t1,\n",
        "                                            pr_y1_t0,\n",
        "                                            pr_l,\n",
        "                                            pr_r,\n",
        "                                            pr_y1_l_t1,\n",
        "                                            pr_y1_l_t0,\n",
        "                                            pr_y1_r_t1,\n",
        "                                            pr_y1_r_t0,\n",
        "                                            pr_t1,\n",
        "                                            pr_t0,\n",
        "                                            pr_l_t1,\n",
        "                                            pr_l_t0)\n",
        "        elif method.lower() == 'chisq':\n",
        "            tmp['info_gain'] = chisq(tmp,\n",
        "                                    pr_y1_t1,\n",
        "                                    pr_y1_t0,\n",
        "                                    pr_l,\n",
        "                                    pr_r,\n",
        "                                    pr_y1_l_t1,\n",
        "                                    pr_y1_l_t0,\n",
        "                                    pr_y1_r_t1,\n",
        "                                    pr_y1_r_t0,\n",
        "                                    pr_t1,\n",
        "                                    pr_t0,\n",
        "                                    pr_l_t1,\n",
        "                                    pr_l_t0)\n",
        "        elif method.lower() == 'int':\n",
        "            tmp['info_gain'] = interaction_split(tmp,\n",
        "                                                pr_y1_t1,\n",
        "                                                pr_y1_t0,\n",
        "                                                pr_l,\n",
        "                                                pr_r,\n",
        "                                                pr_y1_l_t1,\n",
        "                                                pr_y1_l_t0,\n",
        "                                                pr_y1_r_t1,\n",
        "                                                pr_y1_r_t0,\n",
        "                                                pr_t1,\n",
        "                                                pr_t0,\n",
        "                                                pr_l_t1,\n",
        "                                                pr_l_t0,\n",
        "                                                n_t1_L,\n",
        "                                                n_t0_L,\n",
        "                                                n_t1_R,\n",
        "                                                n_t0_R)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "    \n",
        "    # We will select one rows per one distinct candidate\n",
        "    tmp['dups'] = tmp['thres'].duplicated(keep='last')\n",
        "    tmp['thres_ok'] = (tmp['min_bucket_ok'] & (tmp['dups'] == False))\n",
        "    tmp.dropna(inplace=True)\n",
        "    if sum(tmp['thres_ok']) < 1:\n",
        "        return None\n",
        "\n",
        "    tmp = tmp[tmp['thres_ok']]\n",
        "\n",
        "    return tmp[['thres', 'info_gain']]\n",
        "    \n",
        "\n",
        "\n",
        "def num_class(df, predict_attr, treatment_attr):\n",
        "    \"\"\"\n",
        "    Returns the number of Responders and Non-responders in Treatment and Control group\n",
        "    \"\"\"\n",
        "    tr = df[(df[predict_attr] == 1) & (df[treatment_attr] == 1)]  # Responders in Treatment group\n",
        "    tn = df[(df[predict_attr] == 0) & (df[treatment_attr] == 1)]  # Non-responders in Treatment group\n",
        "    cr = df[(df[predict_attr] == 1) & (df[treatment_attr] == 0)]  # Responders in Control group\n",
        "    cn = df[(df[predict_attr] == 0) & (df[treatment_attr] == 0)]  # Non-responders in Control group\n",
        "    return tr.shape[0], tn.shape[0], cr.shape[0], cn.shape[0]\n",
        "\n",
        "\n",
        "def choose_attr(df, attributes, predict_attr, treatment_attr,\n",
        "                method, min_bucket_t0, min_bucket_t1):\n",
        "    \"\"\"\n",
        "    Chooses the attribute and its threshold with the highest info gain\n",
        "    from the set of attributes\n",
        "    \"\"\"\n",
        "    max_info_gain = 0\n",
        "    best_attr = None\n",
        "    threshold = None\n",
        "    # Test each attribute (note attributes maybe be chosen more than once)\n",
        "    for attr in attributes:\n",
        "        df_ig = info_gain(df, attr, predict_attr, treatment_attr,\n",
        "                          method, min_bucket_t0, min_bucket_t1)\n",
        "        if df_ig is None:\n",
        "            continue\n",
        "\n",
        "        # Get the possible indices of maximum info gain\n",
        "        ig = max(df_ig['info_gain'])\n",
        "        idx_ig = df_ig.index[df_ig['info_gain']==ig]\n",
        "        # Break ties randomly\n",
        "        idx_ig = random.choice(idx_ig)\n",
        "        # Get information gain & threshold of that\n",
        "        thres = df_ig['thres'][idx_ig]\n",
        "\n",
        "        if ig > max_info_gain:\n",
        "            max_info_gain = ig\n",
        "            best_attr = attr\n",
        "            threshold = thres\n",
        "    return best_attr, threshold\n",
        "\n",
        "\n",
        "def build_tree(df, cols, predict_attr='Y', treatment_attr='T',\n",
        "               method='ED', depth=1, max_depth=float('INF'),\n",
        "               min_split=2000, min_bucket_t0=None, min_bucket_t1=None,\n",
        "               mtry=None, random_seed=3126):\n",
        "    \"\"\"\n",
        "    Builds the Decision Tree based on training data, attributes to train on,\n",
        "    and a prediction attribute\n",
        "    \"\"\"\n",
        "    if depth == 1:\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    if mtry is None:\n",
        "        mtry = math.floor(math.sqrt(len(cols)))\n",
        "    if min_bucket_t0 is None:\n",
        "        min_bucket_t0 = round(min_split/4)\n",
        "    if min_bucket_t1 is None:\n",
        "        min_bucket_t1 = round(min_split/4)\n",
        "    \n",
        "    # Get the number of positive and negative examples in the training data\n",
        "    tr, tn, cr, cn = num_class(df, predict_attr, treatment_attr)\n",
        "    r_y1_ct1 = tr / (tr + tn)\n",
        "    r_y1_ct0 = cr / (cr + cn)\n",
        "\n",
        "    # Check varialbes have less than 2 levels at the current node\n",
        "    # If not, exclude them as candidates for mtry selection\n",
        "    # To split the node, sum(ok_vars) should be equal or larger than self.mtry\n",
        "    ok_vars = []\n",
        "    for col in cols:\n",
        "        ok_vars.append(len(set(df[col])) > 1)\n",
        "\n",
        "    # Whether we have to split this node\n",
        "    #   1. min split condition: Both the sizes of treatment and control group \n",
        "    #     of an internal node should be larger than 'min_split'\n",
        "    #   2. max depth condition: The depth of tree is 'max_depth'\n",
        "    #   3. min_bucket condition: The number of treatment/control group of a\n",
        "    #     node should be larger than 'min_bucket_t0'/'min_bucket_t1'\n",
        "    #   4. Expected return should be larger than 0 and smaller than 1\n",
        "    #     (for KL-divergence & Chisq splitting criteria)\n",
        "    split_cond = tr + tn > min_split and cr + cn > min_split \\\n",
        "            and 0 < r_y1_ct1 < 1 and 0 < r_y1_ct0 < 1 \\\n",
        "            and depth < max_depth and sum(ok_vars) >= mtry\n",
        "    \n",
        "    best_attr, threshold = None, None\n",
        "    if split_cond:\n",
        "        # Sample columns\n",
        "        ok_cols = [col for col in cols if len(set(df[col])) > 1]\n",
        "        ok_cols = np.random.choice(ok_cols, mtry, replace=False)\n",
        "        # Determine attribute and its threshold value with the highest\n",
        "        # information gain\n",
        "        best_attr, threshold = choose_attr(df, ok_cols, predict_attr, treatment_attr,\n",
        "                                           method, min_bucket_t0, min_bucket_t1)\n",
        "    if best_attr is None:\n",
        "        # Create a leaf node indicating it's prediction\n",
        "        leaf = Node(None,None)\n",
        "        leaf.leaf = True\n",
        "        leaf.predict = (tr / (tr + tn), cr / (cr + cn))\n",
        "        return leaf\n",
        "    else:\n",
        "        # Create internal tree node based on attribute and it's threshold\n",
        "        sub_1 = df[df[best_attr] <= threshold]\n",
        "        sub_2 = df[df[best_attr] > threshold]\n",
        "        sub1_tr, sub1_tn, sub1_cr, sub1_cn = num_class(sub_1, predict_attr, treatment_attr)\n",
        "        sub2_tr, sub2_tn, sub2_cr, sub2_cn = num_class(sub_2, predict_attr, treatment_attr)\n",
        "        tree = Node(best_attr, threshold)\n",
        "        # Recursively build left and right subtree\n",
        "        tree.left = build_tree(sub_1, cols, predict_attr, treatment_attr,\n",
        "                               method=method, depth=depth+1, max_depth=max_depth,\n",
        "                               min_split=min_split, min_bucket_t0=min_bucket_t0, \n",
        "                               min_bucket_t1=min_bucket_t1, mtry=mtry)\n",
        "        tree.right = build_tree(sub_2, cols, predict_attr, treatment_attr,\n",
        "                               method=method, depth=depth+1, max_depth=max_depth,\n",
        "                               min_split=min_split, min_bucket_t0=min_bucket_t0, \n",
        "                               min_bucket_t1=min_bucket_t1, mtry=mtry)\n",
        "        return tree\n",
        "\n",
        "\n",
        "def predict(node, row_df):\n",
        "    \"\"\"\n",
        "    Given a instance of a training data, make a prediction of an observation (row)\n",
        "    based on the Decision Tree\n",
        "    Assumes all data has been cleaned (i.e. no NULL data)\n",
        "    \"\"\"\n",
        "    # If we are at a leaf node, return the prediction of the leaf node\n",
        "    if node.leaf:\n",
        "        return node.predict\n",
        "    # Traverse left or right subtree based on instance's data\n",
        "    if row_df[node.attr] <= node.thres:\n",
        "        return predict(node.left, row_df)\n",
        "    elif row_df[node.attr] > node.thres:\n",
        "        return predict(node.right, row_df)\n",
        "\n",
        "\n",
        "def test_predictions(root, df):\n",
        "    \"\"\"\n",
        "    Given a set of data, make a prediction for each instance using the Decision Tree\n",
        "    \"\"\"\n",
        "    pred_treat = []\n",
        "    pred_control = []\n",
        "    for index,row in df.iterrows():\n",
        "        return_treated, return_control = predict(root, row)\n",
        "        pred_treat.append(return_treated)\n",
        "        pred_control.append(return_control)\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZZouB440A56W",
        "colab": {}
      },
      "source": [
        "def eucli_dist(small_df,\n",
        "               pr_y1_ct1,\n",
        "               pr_y1_ct0,\n",
        "               pr_l,\n",
        "               pr_r,\n",
        "               pr_y1_l_ct1,\n",
        "               pr_y1_l_ct0,\n",
        "               pr_y1_r_ct1,\n",
        "               pr_y1_r_ct0,\n",
        "               pr_ct1,\n",
        "               pr_ct0,\n",
        "               pr_l_ct1,\n",
        "               pr_l_ct0):\n",
        "    # Euclidean gain\n",
        "    ed_node = (pr_y1_ct1 - pr_y1_ct0) ** 2 + ((1 - pr_y1_ct1) - (1 - pr_y1_ct0)) ** 2\n",
        "    ed_l = (pr_y1_l_ct1 - pr_y1_l_ct0) ** 2 + ((1 - pr_y1_l_ct1) - (1 - pr_y1_l_ct0)) ** 2\n",
        "    ed_r = (pr_y1_r_ct1 - pr_y1_r_ct0) ** 2 + ((1 - pr_y1_r_ct1) - (1 - pr_y1_r_ct0)) ** 2\n",
        "    ed_lr = pr_l * ed_l + pr_r * ed_r\n",
        "    ed_gain = ed_lr - ed_node\n",
        "\n",
        "    # Euclidean Normalization factor\n",
        "    gini_ct = 2 * pr_ct1 * (1 - pr_ct1)\n",
        "    ed_ct = (pr_l_ct1 - pr_l_ct0) ** 2 + ((1 - pr_l_ct1) - (1 - pr_l_ct0)) ** 2\n",
        "    gini_ct1 = 2 * pr_l_ct1 * (1 - pr_l_ct1)\n",
        "    gini_ct0 = 2 * pr_l_ct0 * (1 - pr_l_ct0)\n",
        "    ed_norm = gini_ct * ed_ct + gini_ct1 * pr_ct1 + gini_ct0 * pr_ct0 + 0.5\n",
        "    \n",
        "    # Output\n",
        "    info_gain_t = ed_gain / ed_norm\n",
        "\n",
        "    return info_gain_t\n",
        "\n",
        "\n",
        "def kl_divergence(small_df,\n",
        "                  pr_y1_ct1,\n",
        "                  pr_y1_ct0,\n",
        "                  pr_l,\n",
        "                  pr_r,\n",
        "                  pr_y1_l_ct1,\n",
        "                  pr_y1_l_ct0,\n",
        "                  pr_y1_r_ct1,\n",
        "                  pr_y1_r_ct0,\n",
        "                  pr_ct1,\n",
        "                  pr_ct0,\n",
        "                  pr_l_ct1,\n",
        "                  pr_l_ct0):\n",
        "    # KL Gain\n",
        "    kl_node = pr_y1_ct1 * np.log2(pr_y1_ct1/pr_y1_ct0) + \\\n",
        "             (1 - pr_y1_ct1) * np.log2((1 - pr_y1_ct1) / (1 - pr_y1_ct0))\n",
        "    kl_l = pr_y1_l_ct1 * np.log2(pr_y1_l_ct1 / pr_y1_l_ct0) + \\\n",
        "          (1 - pr_y1_l_ct1) * np.log2((1 - pr_y1_l_ct1) / (1 - pr_y1_l_ct0))\n",
        "    kl_r = pr_y1_r_ct1 * np.log2(pr_y1_r_ct1 / pr_y1_r_ct0) + \\\n",
        "          (1 - pr_y1_r_ct1) * np.log2((1 - pr_y1_r_ct1) / (1 - pr_y1_r_ct0))\n",
        "    kl_lr = pr_l * kl_l + pr_r * kl_r\n",
        "    kl_gain = kl_lr - kl_node\n",
        "\n",
        "    # KL Normalization factor\n",
        "    ent_ct = -(pr_ct1 * np.log2(pr_ct1) + pr_ct0 * np.log2(pr_ct0))\n",
        "    kl_ct = pr_l_ct1 * np.log2(pr_l_ct1 / pr_l_ct0) + \\\n",
        "           (1 - pr_l_ct1) * np.log2 ((1 - pr_l_ct1) / (1 - pr_l_ct0))\n",
        "    ent_ct1 = -(pr_l_ct1 * np.log2(pr_l_ct1) + (1 - pr_l_ct1) * np.log2((1 - pr_l_ct1)))\n",
        "    ent_ct0 = -(pr_l_ct0 * np.log2(pr_l_ct0) + (1 - pr_l_ct0) * np.log2((1 - pr_l_ct0)))\n",
        "\n",
        "    norm = kl_ct * ent_ct + ent_ct1 * pr_ct1 + ent_ct0 * pr_ct0 + 0.5\n",
        "\n",
        "    # Output\n",
        "    info_gain_t = kl_gain / norm\n",
        "\n",
        "    return info_gain_t\n",
        "\n",
        "\n",
        "def chisq(small_df,\n",
        "          pr_y1_ct1,\n",
        "          pr_y1_ct0,\n",
        "          pr_l,\n",
        "          pr_r,\n",
        "          pr_y1_l_ct1,\n",
        "          pr_y1_l_ct0,\n",
        "          pr_y1_r_ct1,\n",
        "          pr_y1_r_ct0,\n",
        "          pr_ct1,\n",
        "          pr_ct0,\n",
        "          pr_l_ct1,\n",
        "          pr_l_ct0):\n",
        "    # Chi-squared gain\n",
        "    chisq_node = ((pr_y1_ct1 - pr_y1_ct0) ** 2) / pr_y1_ct0 + \\\n",
        "                (((1 - pr_y1_ct1) - (1 - pr_y1_ct0)) ** 2) / (1 - pr_y1_ct0) \n",
        "    chisq_l = ((pr_y1_l_ct1 - pr_y1_l_ct0) ** 2) / pr_y1_l_ct0 + \\\n",
        "             (((1 - pr_y1_l_ct1) - (1 - pr_y1_l_ct0)) ** 2) / (1 - pr_y1_l_ct0)\n",
        "    chisq_r = ((pr_y1_r_ct1 - pr_y1_r_ct0) ** 2) / pr_y1_r_ct0 + \\\n",
        "             (((1 - pr_y1_r_ct1) - (1 - pr_y1_r_ct0)) ** 2) / (1 - pr_y1_r_ct0)\n",
        "    chisq_lr = pr_l * chisq_l + pr_r * chisq_r\n",
        "    chisq_gain = chisq_lr - chisq_node\n",
        "\n",
        "    # Chi-squared Normalization factor\n",
        "    gini_ct = 2 * pr_ct1 * (1 - pr_ct1) \n",
        "    chisq_ct = ((pr_l_ct1 - pr_l_ct0) ** 2) / pr_l_ct0 + \\\n",
        "              (((1 - pr_l_ct1) - (1 - pr_l_ct0)) ** 2) / (1 - pr_l_ct0)\n",
        "    gini_ct1 = 2 * pr_l_ct1 * (1 - pr_l_ct1)\n",
        "    gini_ct0 = 2 * pr_l_ct0 * (1 - pr_l_ct0)\n",
        "    chisq_norm = gini_ct * chisq_ct + gini_ct1 * pr_ct1  + gini_ct0 * pr_ct0 + 0.5\n",
        "     \n",
        "    # Output\n",
        "    info_gain_t = chisq_gain / chisq_norm\n",
        "\n",
        "    return info_gain_t\n",
        "\n",
        "\n",
        "def interaction_split(small_df,\n",
        "                      pr_y1_ct1,\n",
        "                      pr_y1_ct0,\n",
        "                      pr_l,\n",
        "                      pr_r,\n",
        "                      pr_y1_l_ct1,\n",
        "                      pr_y1_l_ct0,\n",
        "                      pr_y1_r_ct1,\n",
        "                      pr_y1_r_ct0,\n",
        "                      pr_ct1,\n",
        "                      pr_ct0,\n",
        "                      pr_l_ct1,\n",
        "                      pr_l_ct0,\n",
        "                      cs_ct1,\n",
        "                      cs_ct0,\n",
        "                      ncs_ct1,\n",
        "                      ncs_ct0):\n",
        "    # Compute elements for split formula\n",
        "    C44 = 1/cs_ct1 + 1/cs_ct0 + 1/ncs_ct1 + 1/ncs_ct0\n",
        "\n",
        "    UR = pr_y1_r_ct1 - pr_y1_r_ct0\n",
        "    UL = pr_y1_l_ct1 - pr_y1_l_ct0\n",
        "\n",
        "    SSE = cs_ct1 * pr_y1_l_ct1 * (1 - pr_y1_l_ct1) + \\\n",
        "         ncs_ct1 * pr_y1_r_ct1 * (1 - pr_y1_r_ct1) + \\\n",
        "         cs_ct0 * pr_y1_l_ct0 * (1 - pr_y1_l_ct0)  + \\\n",
        "         ncs_ct0 * pr_y1_r_ct0 * (1 - pr_y1_r_ct0)\n",
        "         \n",
        "    n_node = len(small_df)       \n",
        "\n",
        "    # Output: Interaction split\n",
        "    info_gain_t = ((n_node - 4) * (UR - UL)**2) / (C44 * SSE)\n",
        "\n",
        "    return info_gain_t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ORX-_PRzfNw",
        "colab": {}
      },
      "source": [
        "def uplift_tree(x, y, t, **kwargs):\n",
        "    predict_attr = kwargs.get('predict_attr', 'Y')\n",
        "    treatment_attr = kwargs.get('treatment_attr', 'T')\n",
        "    \n",
        "    df = x.copy()\n",
        "    df[predict_attr] = y\n",
        "    df[treatment_attr] = t\n",
        "    \n",
        "    kwargs['predict_attr'] = predict_attr\n",
        "    kwargs['treatment_attr'] = treatment_attr\n",
        "    root = build_tree(df, x.columns, **kwargs)\n",
        "    \n",
        "    return root\n",
        "\n",
        "\n",
        "def predict_tree(root, newdata, **kwargs):\n",
        "    return test_predictions(root, newdata)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EC1PVHpyg7jG",
        "colab": {}
      },
      "source": [
        "def upliftRF(x, y, t, ntree=10, bagging_fraction=0.6, random_seed=3126, **kwargs):\n",
        "    predict_attr = kwargs.get('predict_attr', 'Y')\n",
        "    treatment_attr = kwargs.get('treatment_attr', 'T')\n",
        "    \n",
        "    df = x.copy()\n",
        "    df[predict_attr] = y\n",
        "    df[treatment_attr] = t\n",
        "    \n",
        "    kwargs['predict_attr'] = predict_attr\n",
        "    kwargs['treatment_attr'] = treatment_attr\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "    random_seeds = [np.random.randint(10000) for _ in range(ntree)]\n",
        "    trees = []\n",
        "    for i in range(ntree):\n",
        "        bagged_df = df.sample(frac=bagging_fraction, random_state=random_seeds[i])\n",
        "        trees.append(build_tree(bagged_df, x.columns, random_seed=random_seeds[i], **kwargs))\n",
        "    \n",
        "    return trees\n",
        "\n",
        "\n",
        "def predict_upliftRF(obj, newdata, **kwargs):\n",
        "    pred_trees = []\n",
        "    for tree in obj:\n",
        "        pred_trees.append(test_predictions(tree, newdata))\n",
        "\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": sum([x['pr_y1_t1'] for x in pred_trees])/len(pred_trees),\n",
        "        \"pr_y1_t0\": sum([x['pr_y1_t0'] for x in pred_trees])/len(pred_trees),\n",
        "    })\n",
        "    return pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aOPp6WQ7rFNF",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def z_assign(y, t):\n",
        "    \"\"\" Define transformed response variable z\n",
        "    if (treated and response) or (not treated and not response), return 1\n",
        "    else, return 0\n",
        "    \"\"\"\n",
        "    if y == 1 and t == 1:\n",
        "        return 1\n",
        "    elif y == 0 and t == 1:\n",
        "        return 0\n",
        "    elif y == 1 and t == 0:\n",
        "        return 0\n",
        "    elif y == 0 and t == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def lai(x, y, t, method=GradientBoostingClassifier, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Lai's Approach\" \n",
        "    The default model is Gradient Boosting Machine (gbm)\n",
        "\n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        A sklearn model.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({'y': y.copy()})\n",
        "    df['t'] = t\n",
        "    z = df.apply(lambda row: z_assign(row['y'], row['t']), axis=1)\n",
        "    \n",
        "    model = method(**kwargs).fit(x, z)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_lai(obj, newdata, y, t, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Lai's Approach\" \n",
        "    \n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        obj: A sklearn model.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        dataframe: A dataframe with predictions for when the instances are\n",
        "            treated and for when they are not treated.\n",
        "    \"\"\"\n",
        " \n",
        "    pred = obj.predict_proba(newdata)    # list of [False, True]\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"pr_y1_t1\": [row[1] for row in pred],\n",
        "        \"pr_y1_t0\": [row[0] for row in pred],\n",
        "    })\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c0psF5SuwgWP",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def glai(x, y, t, method=GradientBoostingClassifier, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Lai's Approach\" \n",
        "    The default model is Gradient Boosting Machine (gbm)\n",
        "\n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        A sklearn model.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({'y': y.copy()})\n",
        "    df['t'] = t\n",
        "    ty = df.apply(lambda row: ty_assign(row['y'], row['t']), axis=1)\n",
        "    \n",
        "    model = method(**kwargs).fit(x, ty)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_glai(obj, newdata, y, t, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Lai's Approach\" \n",
        "    \n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        obj: A sklearn model.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        dataframe: A dataframe with predictions for when the instances are\n",
        "            treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "    prob_T = sum(t) / len(t)\n",
        "    prob_C = 1 - prob_T\n",
        "    \n",
        "    pred = obj.predict_proba(newdata)    # list of [CN, CR, TN, TR]\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"pr_y1_t1\": [row[3]/prob_T + row[0]/prob_C for row in pred],   # TR/T + CN/C\n",
        "        \"pr_y1_t0\": [row[2]/prob_T + row[1]/prob_C for row in pred],   # TN/T + CR/C\n",
        "    })\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lqztMu4voFLV",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def z_assign(y, t):\n",
        "    \"\"\" Define transformed response variable z\n",
        "    if (treated and response) or (not treated and not response), return 1\n",
        "    else, return 0\n",
        "    \"\"\"\n",
        "    if y == 1 and t == 1:\n",
        "        return 1\n",
        "    elif y == 0 and t == 1:\n",
        "        return 0\n",
        "    elif y == 1 and t == 0:\n",
        "        return 0\n",
        "    elif y == 0 and t == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rvtu(x, y, t, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Transforming the data according to the \"Jaskowski's Approach\"\n",
        "    Sometimes, it called Response Variable Transformation for Uplift (RVTU)\n",
        "\n",
        "    Source: \"Uplift modeling for clinical trial data\" (Jaskowski, 2006)\n",
        "    \"\"\"\n",
        "\n",
        "    ### Combine x, y, and ct\n",
        "    df = x.copy()\n",
        "    df['y'] = y\n",
        "    df['ct'] = t\n",
        "    df['z'] = df.apply(lambda row: z_assign(row['y'], row['ct']), axis=1)\n",
        "    mdl = method(**kwargs).fit(x, df['z'])\n",
        "    \n",
        "    return mdl\n",
        "\n",
        "\n",
        "def predict_rvtu(obj, newdata, y, t, **kwargs):\n",
        "    # df = pd.DataFrame({'y': y.copy()})\n",
        "    # df['ct'] = ct\n",
        "    # z = df.apply(lambda row: z_assign(row['y'], row['ct']), axis=1)\n",
        "\n",
        "    if isinstance(obj, LinearRegression):\n",
        "        pred = obj.predict(newdata)\n",
        "    else:\n",
        "        pred = obj.predict_proba(newdata)[:, 1]\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"pr_y1_t1\": [row for row in pred],\n",
        "        \"pr_y1_t0\": [1-row for row in pred],\n",
        "    })\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vK0a6vs9ss3R",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def dta(x, y, t, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Dummy Treatment Approach\" \n",
        "    The default model is General Linear Model (GLM)\n",
        "\n",
        "    Source: \"The True Lift Model\" (Lo, 2002)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        A sklearn model.\n",
        "    \"\"\"\n",
        "    # Create interaction variables\n",
        "    # Building our dataframe with the interaction variables\n",
        "    df = x.copy()\n",
        "    for colname in x.columns:\n",
        "        df[\"Int_\" + colname] = x[colname] * t\n",
        "    df['treated'] = t\n",
        "\n",
        "    # Fit a model\n",
        "    model = method(**kwargs).fit(df, y)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_dta(obj, newdata, y_name='y', t_name='treated', **kwargs):\n",
        "    \"\"\"Predictions according to the \"Dummy Treatment Approach\" \n",
        "    \n",
        "    For each instance in newdata two predictions are made:\n",
        "    1) What is the probability of a person responding when treated?\n",
        "    2) What is the probability of a person responding when not treated\n",
        "      (i.e. part of control group)?\n",
        "\n",
        "    Source: \"The True Lift Model\" (Lo, 2002)\n",
        "\n",
        "    Args:\n",
        "        obj: A sklearn model.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        dataframe: A dataframe with predictions for when the instances are\n",
        "            treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "    predictors = [c for c in newdata.columns if c not in (y_name, t_name)]\n",
        "\n",
        "    df_treat = newdata.copy()\n",
        "    df_control = newdata.copy()\n",
        "    for colname in predictors:\n",
        "        df_treat[\"Int_\" + colname] = df_treat[colname] * 1\n",
        "        df_control[\"Int_\" + colname] = df_control[colname] * 0\n",
        "    df_treat['treated'] = 1\n",
        "    df_control['treated'] = 0\n",
        "\n",
        "    # print(obj.coef_, obj.intercept_)\n",
        "    if isinstance(obj, LinearRegression):\n",
        "        pred_treat = obj.predict(df_treat)\n",
        "        pred_control = obj.predict(df_control)\n",
        "    else:\n",
        "#         pred = obj.predict_proba(newdata)[:, 1]\n",
        "        pred_treat = obj.predict_proba(df_treat)[:, 1]\n",
        "        pred_control = obj.predict_proba(df_control)[:, 1]\n",
        "\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExU6BW9etUpx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}