{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_day4_ALL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ehi1150I7fxN",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hH-OAGszyef5",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "link_hilstrom = 'https://drive.google.com/open?id=15osyN4c5z1pSo1JkxwL_N8bZTksRvQuU'\n",
        "fluff, id = link_hilstrom.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Hillstrom.csv')\n",
        "hillstrom_df = pd.read_csv('Hillstrom.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7tAS92JMPe9U",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "link_lalonde = 'https://drive.google.com/open?id=1b8N7WtwIe2WmQJD1KL5UAy70K13MxwKj'\n",
        "fluff, id = link_lalonde.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Lalonde.csv')\n",
        "lalonde_df = pd.read_csv('Lalonde.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyJ1oeuTMFhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link_criteo_fix = 'https://drive.google.com/open?id=13pNFAeH1ZEPxaCU9tQz1H17L28aTcppg'\n",
        "fluff, id = link_criteo_fix.split('=')\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('criteo_small_fix.csv')\n",
        "criteo_df = pd.read_csv('criteo_small_fix.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZoNrZI5P80wJ",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "\n",
        "def preprocess_data(df, dataset='hillstrom', verbose=True):\n",
        "    # For Hillstrom dataset, the ‘‘visit’’ target variable was selected\n",
        "    #   as the target variable of interest and the selected treatment is \n",
        "    #   the e-mail campaign for women’s merchandise [1]\n",
        "    # [1] Kane K, Lo VSY, Zheng J. True-lift modeling: Comparison of methods. \n",
        "    #    J Market Anal. 2014;2:218–238\n",
        "    dataset = dataset.lower()\n",
        "    if dataset in ('hillstrom', 'email'):\n",
        "        columns = df.columns\n",
        "        for col in columns:\n",
        "            if df[col].dtype != object:\n",
        "                continue\n",
        "            df = pd.concat(\n",
        "                    [df, pd.get_dummies(df[col], \n",
        "                                        prefix=col, \n",
        "                                        drop_first=False)],\n",
        "                    axis=1)\n",
        "            df.drop([col], axis=1, inplace=True)\n",
        "\n",
        "        df.columns = [col.replace('-', '').replace(' ', '_').lower()\n",
        "                      for col in df.columns]\n",
        "        df = df[df.segment_mens_email == 0]\n",
        "        df.index = range(len(df))\n",
        "        df.drop(['segment_mens_email', \n",
        "                 'segment_no_email', \n",
        "                 'conversion', \n",
        "                 'spend'], axis=1, inplace=True)\n",
        "\n",
        "        y_name = 'visit'\n",
        "        t_name = 'segment_womens_email'\n",
        "    elif dataset in ['criteo', 'ad']:\n",
        "        df = df.fillna(0)\n",
        "        y_name = 'y'\n",
        "        t_name = 'treatment'\n",
        "    elif dataset == 'lalonde':\n",
        "        y_name = 'RE78'\n",
        "        t_name = 'treatment'\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    df['Y'] = df[y_name]\n",
        "    df.drop([y_name], axis=1, inplace=True)\n",
        "    df['T'] = df[t_name]\n",
        "    df.drop([t_name], axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Igf3QLgdJ1cW",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def performance(pr_y1_t1, pr_y1_t0, y, t, groups=10):\n",
        "    \"\"\"\n",
        "    1. Split the total customers into the given number of groups\n",
        "    2. Calculate the statistics of each segment\n",
        "    \n",
        "    Args:\n",
        "        pr_y1_t1: the series (list) of the customer's expected return\n",
        "        pr_y1_t0: the expected return when a customer is not treated\n",
        "        y: the observed return of customers\n",
        "        t: whther each customer is treated or not\n",
        "        groups: the number of groups (segments). Should be 5, 10, or 20\n",
        "    Return:\n",
        "        DataFrame:\n",
        "            columns:\n",
        "                'n_y1_t1': the number of treated responders\n",
        "                'n_y1_t0': the number of not treated responders\n",
        "                'r_y1_t1': the average return of treated customers\n",
        "                'r_y1_t0': the average return of not treated customers\n",
        "                'n_t1': the number of treated customers\n",
        "                'n_t0': the number of not treated customers\n",
        "                'uplift': the average uplift (the average treatment effect)\n",
        "            rows: the index of groups\n",
        "    \"\"\"\n",
        "  \n",
        "    ### check valid arguments\n",
        "    if groups not in [5, 10, 20]:\n",
        "        raise Exception(\"uplift: groups must be either 5, 10 or 20\")\n",
        "  \n",
        "    ### check for NAs.\n",
        "    if pr_y1_t1.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in pr_y1_t1\")\n",
        "    if pr_y1_t0.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in pr_y1_t0\")\n",
        "    if y.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in y\")\n",
        "    if t.isnull().values.any():\n",
        "        raise Exception(\"uplift: NA not permitted in t\")\n",
        "   \n",
        "    ### check valid values for y and t\n",
        "    # if set(y) != {0, 1}:\n",
        "    #     raise Exception(\"uplift: y must be either 0 or 1\")\n",
        "    if set(t) != {0, 1}:\n",
        "        raise Exception(\"uplift: t must be either 0 or 1\")\n",
        "\n",
        "    ### check length of arguments\n",
        "    if not (len(pr_y1_t1) == len(pr_y1_t0) == len(y) == len(t)):\n",
        "        raise Exception(\"uplift: arguments pr_y1_t1, pr_y1_t0, y and t must all have the same length\")\n",
        "\n",
        "    ### define dif_pred\n",
        "    dif_pred = pr_y1_t1 - pr_y1_t0\n",
        "  \n",
        "    ### Make index same\n",
        "    y.index = dif_pred.index\n",
        "    t.index = dif_pred.index\n",
        "    \n",
        "    mm = pd.DataFrame({\n",
        "        'dif_pred': dif_pred,\n",
        "        'y': y,\n",
        "        't': t,\n",
        "        'dif_pred_r': dif_pred.rank(ascending=False, method='first')\n",
        "    })\n",
        "\n",
        "    mm_groupby = mm.groupby(pd.qcut(mm['dif_pred_r'], groups, labels=range(1, groups+1), duplicates='drop'))\n",
        "  \n",
        "    n_y1_t1 = mm_groupby.apply(lambda r: r[r['t'] == 1]['y'].sum())\n",
        "    n_y1_t0 = mm_groupby.apply(lambda r: r[r['t'] == 0]['y'].sum())\n",
        "    n_t1 = mm_groupby['t'].sum()\n",
        "    n_t0 = mm_groupby['t'].count() - n_t1\n",
        "  \n",
        "    df = pd.DataFrame({\n",
        "        'n_t1': n_t1,\n",
        "        'n_t0': n_t0,\n",
        "        'n_y1_t1': n_y1_t1,\n",
        "        'n_y1_t0': n_y1_t0,\n",
        "        'r_y1_t1': n_y1_t1 / n_t1,\n",
        "        'r_y1_t0': n_y1_t0 / n_t0,\n",
        "    })\n",
        "    fillna_columns = ['n_y1_t1', 'n_y1_t0', 'n_t1', 'n_t0']\n",
        "    df[fillna_columns] = df[fillna_columns].fillna(0)\n",
        "    df.index.name = 'groups'\n",
        "\n",
        "    df['uplift'] = df['r_y1_t1'] - df['r_y1_t0']\n",
        "    df['uplift'] = round(df['uplift'], 6)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def qini(perf, plotit=True):\n",
        "    nrow = len(perf)\n",
        "\n",
        "    # Calculating the incremental gains. \n",
        "    # - First, the cumulitative sum of the treated and the control groups are\n",
        "    #  calculated with respect to the total population in each group at the\n",
        "    #  specified decile\n",
        "    # - Afterwards we calculate the percentage of the total amount of people\n",
        "    #  (both treatment and control) are present in each decile\n",
        "    cumul_y1_t1 = (perf['n_y1_t1'].cumsum() / perf['n_t1'].cumsum()).fillna(0)\n",
        "    cumul_y1_t0 = (perf['n_y1_t0'].cumsum() / perf['n_t0'].cumsum()).fillna(0)\n",
        "    deciles = [i/nrow for i in range(1, nrow+1)]\n",
        "\n",
        "    ### Model Incremental gains\n",
        "    inc_gains = (cumul_y1_t1 - cumul_y1_t0) * deciles\n",
        "    inc_gains = [0.0] + list(inc_gains)\n",
        "\n",
        "    ### Overall incremental gains\n",
        "    overall_inc_gain = sum(perf['n_y1_t1']) / sum(perf['n_t1']) \\\n",
        "            - sum(perf['n_y1_t0']) / sum(perf['n_t0'])\n",
        "\n",
        "    ### Random incremental gains\n",
        "    random_inc_gains = [i*overall_inc_gain / nrow for i in range(nrow+1)]\n",
        "\n",
        "    ### Compute area under the model incremental gains (uplift) curve\n",
        "    x = [0] + deciles\n",
        "    y = list(inc_gains)\n",
        "    auuc = 0\n",
        "    auuc_rand = 0\n",
        "\n",
        "    auuc_list = [auuc]\n",
        "    for i in range(1, len(x)):\n",
        "        auuc += 0.5 * (x[i] - x[i-1]) * (y[i] + y[i-1])\n",
        "        auuc_list.append(auuc)\n",
        "\n",
        "    ### Compute area under the random incremental gains curve\n",
        "    y_rand = random_inc_gains\n",
        "\n",
        "    auuc_rand_list = [auuc_rand]\n",
        "    for i in range(1, len(x)):\n",
        "        auuc_rand += 0.5 * (x[i] - x[i-1]) * (y_rand[i] + y_rand[i-1])\n",
        "        auuc_rand_list.append(auuc_rand)\n",
        "\n",
        "    ### Compute the difference between the areas (Qini coefficient)\n",
        "    Qini = auuc - auuc_rand\n",
        "\n",
        "    ### Plot incremental gains curve\n",
        "    if plotit:\n",
        "        x_axis = x\n",
        "        plt.plot(x_axis, inc_gains)\n",
        "        plt.plot(x_axis, random_inc_gains)\n",
        "        plt.show()\n",
        "    \n",
        "    ### Qini 30%, Qini 10%\n",
        "    n_30p = int(nrow*3/10)\n",
        "    n_10p = int(nrow/10)\n",
        "    qini_30p = auuc_list[n_30p] - auuc_rand_list[n_30p]\n",
        "    qini_10p = auuc_list[n_10p] - auuc_rand_list[n_10p]\n",
        "\n",
        "    res = {\n",
        "        'qini': Qini,\n",
        "        'inc_gains': inc_gains,\n",
        "        'random_inc_gains': random_inc_gains,\n",
        "        'auuc_list': auuc_list,\n",
        "        'auuc_rand_list': auuc_rand_list,\n",
        "        'qini_30p': qini_30p,\n",
        "        'qini_10p': qini_10p,\n",
        "    }    \n",
        "\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nrxjl1v7J9Mm",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "def parameter_tuning(fit_mdl, pred_mdl, data, search_space, plotit=False):\n",
        "    \"\"\"\n",
        "    Given a model, search all combination of parameter sets and find\n",
        "    the best parameter set\n",
        "    \n",
        "    Args:\n",
        "        fit_mdl: model function\n",
        "        pred_mdl: predict function of fit_mdl\n",
        "        data:\n",
        "            {\n",
        "                \"x_train\": predictor variables of training dataset,\n",
        "                \"y_train\": target variables of training dataset,\n",
        "                \"t_train\": treatment variables of training dataset,\n",
        "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
        "                \"y_test\": target variables of test (usually, validation) dataset,\n",
        "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
        "            }\n",
        "        search_space:\n",
        "            {\n",
        "                parameter_name: [search values]\n",
        "            }\n",
        "    Return:\n",
        "        The best parameter set\n",
        "    \"\"\"\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['y_train']\n",
        "    t_train = data['t_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "    t_test = data['t_test']\n",
        "    \n",
        "    max_q = -float('inf')\n",
        "    best_mdl = None\n",
        "\n",
        "    keys = search_space.keys()\n",
        "    n_space = [len(search_space[key]) for key in keys]\n",
        "    n_iter = np.prod(n_space)\n",
        "    \n",
        "    best_params = None\n",
        "    for i in range(n_iter):\n",
        "        params = {}\n",
        "        for idx, key in enumerate(keys):\n",
        "            params[key] = search_space[key][i % n_space[idx]]\n",
        "            i = int(i / n_space[idx])\n",
        "\n",
        "        mdl = fit_mdl(x_train, y_train, t_train, **params)\n",
        "        pred = pred_mdl(mdl, newdata=x_test, y=y_test, t=t_test, ct=t_test)\n",
        "        # print('    {}'.format(params))\n",
        "        try:\n",
        "            perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "        q = qini(perf, plotit=plotit)['qini']\n",
        "        if plotit:\n",
        "            print(q, params)\n",
        "        if q > max_q:\n",
        "            max_q = q\n",
        "            best_mdl = mdl\n",
        "            best_params = params\n",
        "\n",
        "    return best_mdl, best_params\n",
        "\n",
        "\n",
        "def wrapper(fit_mdl, pred_mdl, data, params=None,\n",
        "            best_models=None, drop_variables=None, qini_values=None, wrapper_variables=None):\n",
        "    \"\"\"\n",
        "    General wrapper approach\n",
        "    \n",
        "    Args:\n",
        "        fit_mdl: model function\n",
        "        pred_mdl: predict function of fit_mdl\n",
        "        data:\n",
        "            {\n",
        "                \"x_train\": predictor variables of training dataset,\n",
        "                \"y_train\": target variables of training dataset,\n",
        "                \"t_train\": treatment variables of training dataset,\n",
        "                \"x_test\": predictor variables of test (usually, validation) dataset,\n",
        "                \"y_test\": target variables of test (usually, validation) dataset,\n",
        "                \"t_test\": treatment variables of test (usually, validation) dataset,\n",
        "            }\n",
        "    Return:\n",
        "        (A list of best models, The list of dropped variables)\n",
        "    \"\"\"\n",
        "    if best_models is None:\n",
        "        best_models = []\n",
        "    if drop_variables is None:\n",
        "        drop_variables = []\n",
        "    if qini_values is None:\n",
        "        qini_values = []\n",
        "    if params is None:\n",
        "        params = {}\n",
        "\n",
        "    x_train = data['x_train']\n",
        "    y_train = data['y_train']\n",
        "    t_train = data['t_train']\n",
        "    x_test = data['x_test']\n",
        "    y_test = data['y_test']\n",
        "    t_test = data['t_test']\n",
        "\n",
        "    variables = data['x_train'].columns\n",
        "\n",
        "    max_q = -float('inf')\n",
        "    drop_var = None\n",
        "    best_mdl = None\n",
        "    for var in variables:\n",
        "        if var in drop_variables:\n",
        "            continue\n",
        "        x = x_train.copy()\n",
        "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
        "        mdl = fit_mdl(x, y_train, t_train, **params)\n",
        "        x = x_test.copy()\n",
        "        x.drop(drop_variables + [var], axis=1, inplace=True)\n",
        "        pred = pred_mdl(mdl, newdata=x, y=y_test, t=t_test, ct=t_test)\n",
        "        perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], y_test, t_test)\n",
        "        q = qini(perf, plotit=False)['qini']\n",
        "        if q > max_q:\n",
        "            max_q = q\n",
        "            drop_var = var\n",
        "            best_mdl = mdl\n",
        "    \n",
        "        previous_max = wrapper_variables.loc[len(drop_variables) + 1, fit_mdl.__name__]\n",
        "        if q > previous_max:\n",
        "            wrapper_variables.loc[len(drop_variables) + 1, fit_mdl.__name__] = q\n",
        "    \n",
        "    \n",
        "    best_models.append(best_mdl)\n",
        "    drop_variables.append(drop_var)\n",
        "    qini_values.append(max_q)\n",
        "\n",
        "    left_vars = [var for var in variables if (var not in drop_variables)]\n",
        "    \n",
        "    if len(variables) == len(drop_variables) + 1:\n",
        "        return best_models, drop_variables + left_vars, qini_values\n",
        "    else:\n",
        "        return wrapper(fit_mdl, pred_mdl, data, params=params,\n",
        "                       best_models=best_models, drop_variables=drop_variables,\n",
        "                       qini_values=qini_values, wrapper_variables=wrapper_variables)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxrEbEODlbQi",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def tma(x, y, t, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Two Model Approach\" \n",
        "    (a.k.a. \"Separate Model Approach\")\n",
        "    The default model is General Linear Model (GLM)\n",
        "    \n",
        "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        Dictionary: A dictionary of two models. One for the treatment group, \n",
        "            one for the control group.\n",
        "\n",
        "            {\n",
        "                'model_treat': a model for the treatment group,\n",
        "                'model_control': a model for the control group\n",
        "            }\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    treat_rows = (t == 1)\n",
        "    control_rows = (t == 0)\n",
        "    model_treat = method(**kwargs).fit(x[treat_rows], y[treat_rows])\n",
        "    model_control = method(**kwargs).fit(x[control_rows], y[control_rows])\n",
        "    \n",
        "    res = {\n",
        "        'model_treat': model_treat,\n",
        "        'model_control': model_control,\n",
        "    }\n",
        "    return res\n",
        "\n",
        "\n",
        "def predict_tma(obj, newdata, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Two Model Approach\" \n",
        "    (a.k.a. \"Separate Model Approach\")\n",
        "    \n",
        "    For each instance in newdata two predictions are made:\n",
        "    1) What is the probability of a person responding when treated?\n",
        "    2) What is the probability of a person responding when not treated\n",
        "      (i.e. part of control group)?\n",
        "\n",
        "    Source: \"Incremental Value Modeling\" (Hansotia, 2002)\n",
        "\n",
        "    Args:\n",
        "        obj: A dictionary of two models. \n",
        "            One for the treatment group, one for the control group.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        DataFrame: A dataframe with predicted returns for when the customers\n",
        "            are treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "   \n",
        "    if isinstance(obj['model_treat'], LinearRegression):\n",
        "        pred_treat = obj['model_treat'].predict(newdata)\n",
        "    else:\n",
        "        pred_treat = obj['model_treat'].predict_proba(newdata)[:, 1]\n",
        "\n",
        "    if isinstance(obj['model_control'], LinearRegression):\n",
        "        pred_control = obj['model_control'].predict(newdata)\n",
        "    else:\n",
        "        pred_control = obj['model_control'].predict_proba(newdata)[:, 1]\n",
        "    \n",
        "    # pred_treat = obj['model_treat'].predict(newdata)\n",
        "    # pred_control = obj['model_control'].predict(newdata)\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hREl_CRv9DYC",
        "outputId": "8bced176-e1ba-40e6-a401-6ea73e46db6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def ty_assign(y, t):\n",
        "    if y == 1 and t == 1:\n",
        "        return \"TR\"\n",
        "    elif y == 0 and t == 1:\n",
        "        return \"TN\"\n",
        "    elif y == 1 and t == 0:\n",
        "        return \"CR\"\n",
        "    elif y == 0 and t == 0:\n",
        "        return \"CN\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def t_assign(ty):\n",
        "    if ty in (\"TR\", \"TN\"):\n",
        "        return 1\n",
        "    elif ty in (\"CR\", \"CN\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def y_assign(ty):\n",
        "    if ty in (\"TR\", \"CR\"):\n",
        "        return 1\n",
        "    elif ty in (\"TN\", \"CN\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "search_space = {\n",
        "    'method': [LogisticRegression],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
        "    'penalty': ['none', 'l2'],\n",
        "    'tol': [1e-2, 1e-3, 1e-4],\n",
        "    'C': [1e6, 1e3, 1, 1e-3, 1e-6],\n",
        "}\n",
        "\n",
        "search_space = {\n",
        "    'ntree': [10, ],\n",
        "    'mtry': [3, ],\n",
        "    'bagging_fraction': [0.6, ],\n",
        "    'method': ['ED',],\n",
        "    'max_depth': [10, ],\n",
        "    'min_split': [1000, ],\n",
        "    'min_bucket_t0': [100,],\n",
        "    'min_bucket_t1': [100,],\n",
        "}\n",
        "\n",
        "search_space = {\n",
        "    'solver': ['liblinear',],\n",
        "}\n",
        "\n",
        "def main():\n",
        "    ### Load data ###\n",
        "#     datasets = [hillstrom_df, lalonde_df, criteo_df]\n",
        "    datasets = [hillstrom_df]\n",
        "#     dataset_type = ['hillstrom', 'lalonde', 'criteo']\n",
        "    dataset_type = ['hillstrom']\n",
        "    target_models = {\n",
        "        'dta': [(dta, predict_dta)],\n",
        "        'tma': [(tma, predict_tma)],\n",
        "        'trans': [(lai, predict_lai), (glai, predict_glai), (rvtu, predict_rvtu)],\n",
        "        'tree': [(uplift_tree, predict_tree), (upliftRF, predict_upliftRF), (rvtu, predict_rvtu)],\n",
        "#         'trans': [(rvtu, predict_rvtu)]\n",
        "    }\n",
        "#     df = pd.read_csv('Hillstrom.csv')\n",
        "#     dataset = 'hillstrom'\n",
        "\n",
        "#     fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "    \n",
        "    for i, dataset in enumerate(datasets):\n",
        "        df = dataset\n",
        "        df = preprocess_data(df, dataset=dataset_type[i])\n",
        "        Y = df['Y']\n",
        "        T = df['T']\n",
        "        X = df.drop(['Y', 'T'], axis=1)\n",
        "        ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
        "               .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)\n",
        "        if dataset_type[i] == 'hillstrom':\n",
        "            fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=3126).split(X, ty)\n",
        "        else:\n",
        "            fold_gen = KFold(n_splits=5, shuffle=True, random_state=3126).split(X)\n",
        "        \n",
        "        wrapper_variables = pd.DataFrame(index = range(len(X.columns)))\n",
        "        qini_list = pd.DataFrame(index = ['qini', 'inc_gains', 'random_inc_gains','auuc_list','auuc_rand_list','qini_30p','qini_10p'])\n",
        "        ### Cross validation ###        \n",
        "        for idx, (train_index, test_index) in enumerate(fold_gen):\n",
        "            X_train = X.reindex(train_index)\n",
        "            X_test = X.reindex(test_index)\n",
        "            Y_train = Y.reindex(train_index)\n",
        "            Y_test = Y.reindex(test_index)\n",
        "            T_train = T.reindex(train_index)\n",
        "            T_test = T.reindex(test_index)\n",
        "\n",
        "            df = X_train.copy()\n",
        "            df['Y'] = Y_train\n",
        "            df['T'] = T_train\n",
        "            stratify = T_train\n",
        "            if dataset_type[i] == 'hillstrom':\n",
        "                stratify = df[['Y', 'T']]\n",
        "            tuning_df, validate_df = train_test_split(\n",
        "              df, test_size=0.33, random_state=3126, stratify=stratify)\n",
        "\n",
        "            X_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
        "            Y_tuning = tuning_df['Y']\n",
        "            T_tuning = tuning_df['T']\n",
        "\n",
        "            X_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
        "            Y_validate = validate_df['Y']\n",
        "            T_validate = validate_df['T']\n",
        "\n",
        "            data_dict = {\n",
        "              \"x_train\": X_tuning,\n",
        "              \"y_train\": Y_tuning,\n",
        "              \"t_train\": T_tuning,\n",
        "              \"x_test\": X_validate,\n",
        "              \"y_test\": Y_validate,\n",
        "              \"t_test\": T_validate,\n",
        "            }\n",
        "\n",
        "            for target_model in target_models:\n",
        "                print(target_model)\n",
        "                for model, predict in target_models[target_model]:\n",
        "                    print(model.__name__)\n",
        "                    if model.__name__ not in wrapper_variables.columns:\n",
        "                        wrapper_variables[model.__name__] = -float('inf')\n",
        "\n",
        "                    model_method = search_space.get('method', None)\n",
        "                    params = {\n",
        "                      'method': None if model_method is None else model_method[0],\n",
        "                    }\n",
        "                    if params['method'] == LogisticRegression:\n",
        "                        solver = search_space.get('solver', None)\n",
        "                        params['solver'] = None if solver is None else solver[0]\n",
        "\n",
        "                    print(\"Start warpper \" + str(idx))\n",
        "                    if target_model != 'Tree'\n",
        "                        _, drop_vars, qini_values = wrapper(\n",
        "                              model, predict, data_dict, params=params, wrapper_variables = wrapper_variables)\n",
        "                        best_qini = max(qini_values)\n",
        "                        best_idx = qini_values.index(best_qini)\n",
        "                        best_drop_vars = drop_vars[:best_idx]\n",
        "\n",
        "                        X_tuning.drop(best_drop_vars, axis=1, inplace=True)\n",
        "                        X_validate.drop(best_drop_vars, axis=1, inplace=True)\n",
        "                        X_train.drop(best_drop_vars, axis=1, inplace=True)\n",
        "                        X_test.drop(best_drop_vars, axis=1, inplace=True)\n",
        "\n",
        "                    _, best_params = parameter_tuning(model, predict, data_dict, \n",
        "                                                    search_space=search_space)\n",
        "                    best_params = {}\n",
        "                    mdl = model(X_train, Y_train, T_train, **best_params)\n",
        "                    pred = predict(mdl, X_test, y=Y_test, t=T_test)\n",
        "                    perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
        "                    q = qini(perf, plotit=False)\n",
        "                    qini_list[model.__name__] = pd.DataFrame(q)\n",
        "                print(wrapper_variables.columns)\n",
        "        \n",
        "        wrapper_variables.plot()\n",
        "        plt.title(\"Variable Selection - Dataset \" + str(idx) + \": \" + dataset_type[i])\n",
        "        plt.xlabel(\"Amount of Variables\")\n",
        "        plt.ylabel(\"Qini Value\")\n",
        "        #         plt.legend(bbox_to_anchor=(1.05, 1), loc=(i+1) * 2 - 1 , borderaxespad=0.)\n",
        "#         print('Qini values: ', qini_list)\n",
        "#         print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n",
        "        plt.show()\n",
        "\n",
        "#         plt.subplot(141)\n",
        "#         plt.plot([1,2,3], label=\"test1\")\n",
        "#         plt.plot([3,2,1], label=\"test2\")\n",
        "#         # Place a legend to the right of this smaller subplot.\n",
        "#         plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "         \n",
        "\n",
        "main()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dta\n",
            "dta\n",
            "Start warpper 0\n",
            "Index(['dta'], dtype='object')\n",
            "tma\n",
            "tma\n",
            "Start warpper 0\n",
            "Index(['dta', 'tma'], dtype='object')\n",
            "trans\n",
            "lai\n",
            "Start warpper 0\n",
            "glai\n",
            "Start warpper 0\n",
            "rvtu\n",
            "Start warpper 0\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "dta\n",
            "dta\n",
            "Start warpper 1\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "tma\n",
            "tma\n",
            "Start warpper 1\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "trans\n",
            "lai\n",
            "Start warpper 1\n",
            "glai\n",
            "Start warpper 1\n",
            "rvtu\n",
            "Start warpper 1\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "dta\n",
            "dta\n",
            "Start warpper 2\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "tma\n",
            "tma\n",
            "Start warpper 2\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "trans\n",
            "lai\n",
            "Start warpper 2\n",
            "glai\n",
            "Start warpper 2\n",
            "rvtu\n",
            "Start warpper 2\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "dta\n",
            "dta\n",
            "Start warpper 3\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "tma\n",
            "tma\n",
            "Start warpper 3\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "trans\n",
            "lai\n",
            "Start warpper 3\n",
            "glai\n",
            "Start warpper 3\n",
            "rvtu\n",
            "Start warpper 3\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "[1 0]\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "dta\n",
            "dta\n",
            "Start warpper 4\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "tma\n",
            "tma\n",
            "Start warpper 4\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "trans\n",
            "lai\n",
            "Start warpper 4\n",
            "glai\n",
            "Start warpper 4\n",
            "rvtu\n",
            "Start warpper 4\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "Index(['dta', 'tma', 'lai', 'glai', 'rvtu'], dtype='object')\n",
            "Qini values:  []\n",
            "    mean: nan, std: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhSUYY4wxQWEJxhhjTFBYgjHGGBMU/x/MbpS9Abq0zwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9X42GUaj-UX",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class Node(object):\n",
        "    def __init__(self, attribute, threshold):\n",
        "        self.attr = attribute\n",
        "        self.thres = threshold\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.leaf = False\n",
        "        self.predict = None, None\n",
        "\n",
        "\n",
        "def info_gain(df, attribute, predict_attr, treatment_attr,\n",
        "              method, min_bucket_t0, min_bucket_t1):\n",
        "    \"\"\"\n",
        "    Select the information gain and threshold of the attribute to split\n",
        "    The threshold chosen splits the test data such that information gain is maximized\n",
        "    \n",
        "    Return a pandas.DataFrame\n",
        "        columns: 'thres' (threshold) and 'info_gain' (information gain)\n",
        "    \"\"\"\n",
        "    num_total = df.shape[0]\n",
        "    tmp = pd.DataFrame({\n",
        "        'thres': df[attribute],\n",
        "        'Y': df[predict_attr],\n",
        "        'T': df[treatment_attr]\n",
        "    })\n",
        "    tmp.sort_values(['thres'], inplace=True)\n",
        "\n",
        "    tmp['n_t1_L'] = (tmp['T']).cumsum()\n",
        "    tmp['n_t0_L'] = (tmp['T'] == 0).cumsum()\n",
        "    tmp['n_t1_R'] = sum(tmp['T']) - (tmp['T']).cumsum()\n",
        "    tmp['n_t0_R'] = sum(tmp['T'] == 0) - (tmp['T'] == 0).cumsum()\n",
        "    tmp['n_y_t1_L'] = (tmp['T'] & tmp['Y']).cumsum()\n",
        "    tmp['n_y_t0_L'] = ((tmp['T'] == 0) & tmp['Y']).cumsum()\n",
        "    tmp['n_y_t1_R'] = sum(tmp['T'] & tmp['Y']) - (tmp['T'] & tmp['Y']).cumsum()\n",
        "    tmp['n_y_t0_R'] = sum((tmp['T'] == 0) & tmp['Y']) - ((tmp['T'] == 0) & tmp['Y']).cumsum()\n",
        "        \n",
        "    # min bucket condition\n",
        "    #   Check the size of treatment & control group in left & right child\n",
        "    tmp['min_bucket_ok'] = ((tmp['n_t1_L'] >= min_bucket_t1) & \n",
        "                            (tmp['n_t0_L'] >= min_bucket_t0) &\n",
        "                            (tmp['n_t1_R'] >= min_bucket_t1) &\n",
        "                            (tmp['n_t0_R'] >= min_bucket_t0))\n",
        "    \n",
        "    if sum(tmp['min_bucket_ok']) > 0:\n",
        "        num_total = df.shape[0]\n",
        "        tr, tn, cr, cn = num_class(df, predict_attr, treatment_attr)\n",
        "        n_t1 = tr + tn\n",
        "        n_t0 = cr + cn\n",
        "        pr_t1 = (tr + tn) / (num_total)\n",
        "        # r_t0 = (tr + cr) / (num_total)\n",
        "        pr_t0 = 1 - pr_t1\n",
        "        pr_y1_t1 = tr / (tr + tn)\n",
        "        pr_y1_t0 = cr / (cr + cn)\n",
        "\n",
        "        # Randomized assignment implies pr_l_t1 = pr_l_t0 for all possible splits\n",
        "        pr_l_t1 = (tmp['n_t1_L']) / (n_t1)\n",
        "        pr_l_t0 = (tmp['n_t0_L']) / (n_t0)\n",
        "        pr_l = pr_l_t1 * pr_t1 + pr_l_t0 * pr_t0\n",
        "        pr_r = 1 - pr_l\n",
        "\n",
        "        # Add Laplace correction to probablities\n",
        "        pr_y1_l_t1 = (tmp['n_y_t1_L']) / (tmp['n_t1_L'])\n",
        "        pr_y1_l_t0 = (tmp['n_y_t0_L']) / (tmp['n_t0_L'])\n",
        "        pr_y1_r_t1 = (tmp['n_y_t1_R']) / (tmp['n_t1_R'])\n",
        "        pr_y1_r_t0 = (tmp['n_y_t0_R']) / (tmp['n_t0_R'])\n",
        "\n",
        "        # Number of treatment/control observations at left and right child nodes\n",
        "        n_t1_L = tmp['n_t1_L']\n",
        "        n_t0_L = tmp['n_t0_L']\n",
        "        n_t1_R = tmp['n_t1_R']\n",
        "        n_t0_R = tmp['n_t0_R']\n",
        "\n",
        "        if method.lower() == 'ed':\n",
        "            tmp['info_gain'] = eucli_dist(tmp,\n",
        "                                          pr_y1_t1,\n",
        "                                          pr_y1_t0,\n",
        "                                          pr_l,\n",
        "                                          pr_r,\n",
        "                                          pr_y1_l_t1,\n",
        "                                          pr_y1_l_t0,\n",
        "                                          pr_y1_r_t1,\n",
        "                                          pr_y1_r_t0,\n",
        "                                          pr_t1,\n",
        "                                          pr_t0,\n",
        "                                          pr_l_t1,\n",
        "                                          pr_l_t0)\n",
        "        elif method.lower() == 'kl':\n",
        "            tmp['info_gain'] = kl_divergence(tmp,\n",
        "                                            pr_y1_t1,\n",
        "                                            pr_y1_t0,\n",
        "                                            pr_l,\n",
        "                                            pr_r,\n",
        "                                            pr_y1_l_t1,\n",
        "                                            pr_y1_l_t0,\n",
        "                                            pr_y1_r_t1,\n",
        "                                            pr_y1_r_t0,\n",
        "                                            pr_t1,\n",
        "                                            pr_t0,\n",
        "                                            pr_l_t1,\n",
        "                                            pr_l_t0)\n",
        "        elif method.lower() == 'chisq':\n",
        "            tmp['info_gain'] = chisq(tmp,\n",
        "                                    pr_y1_t1,\n",
        "                                    pr_y1_t0,\n",
        "                                    pr_l,\n",
        "                                    pr_r,\n",
        "                                    pr_y1_l_t1,\n",
        "                                    pr_y1_l_t0,\n",
        "                                    pr_y1_r_t1,\n",
        "                                    pr_y1_r_t0,\n",
        "                                    pr_t1,\n",
        "                                    pr_t0,\n",
        "                                    pr_l_t1,\n",
        "                                    pr_l_t0)\n",
        "        elif method.lower() == 'int':\n",
        "            tmp['info_gain'] = interaction_split(tmp,\n",
        "                                                pr_y1_t1,\n",
        "                                                pr_y1_t0,\n",
        "                                                pr_l,\n",
        "                                                pr_r,\n",
        "                                                pr_y1_l_t1,\n",
        "                                                pr_y1_l_t0,\n",
        "                                                pr_y1_r_t1,\n",
        "                                                pr_y1_r_t0,\n",
        "                                                pr_t1,\n",
        "                                                pr_t0,\n",
        "                                                pr_l_t1,\n",
        "                                                pr_l_t0,\n",
        "                                                n_t1_L,\n",
        "                                                n_t0_L,\n",
        "                                                n_t1_R,\n",
        "                                                n_t0_R)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "    \n",
        "    # We will select one rows per one distinct candidate\n",
        "    tmp['dups'] = tmp['thres'].duplicated(keep='last')\n",
        "    tmp['thres_ok'] = (tmp['min_bucket_ok'] & (tmp['dups'] == False))\n",
        "    tmp.dropna(inplace=True)\n",
        "    if sum(tmp['thres_ok']) < 1:\n",
        "        return None\n",
        "\n",
        "    tmp = tmp[tmp['thres_ok']]\n",
        "\n",
        "    return tmp[['thres', 'info_gain']]\n",
        "    \n",
        "\n",
        "\n",
        "def num_class(df, predict_attr, treatment_attr):\n",
        "    \"\"\"\n",
        "    Returns the number of Responders and Non-responders in Treatment and Control group\n",
        "    \"\"\"\n",
        "    tr = df[(df[predict_attr] == 1) & (df[treatment_attr] == 1)]  # Responders in Treatment group\n",
        "    tn = df[(df[predict_attr] == 0) & (df[treatment_attr] == 1)]  # Non-responders in Treatment group\n",
        "    cr = df[(df[predict_attr] == 1) & (df[treatment_attr] == 0)]  # Responders in Control group\n",
        "    cn = df[(df[predict_attr] == 0) & (df[treatment_attr] == 0)]  # Non-responders in Control group\n",
        "    return tr.shape[0], tn.shape[0], cr.shape[0], cn.shape[0]\n",
        "\n",
        "\n",
        "def choose_attr(df, attributes, predict_attr, treatment_attr,\n",
        "                method, min_bucket_t0, min_bucket_t1):\n",
        "    \"\"\"\n",
        "    Chooses the attribute and its threshold with the highest info gain\n",
        "    from the set of attributes\n",
        "    \"\"\"\n",
        "    max_info_gain = 0\n",
        "    best_attr = None\n",
        "    threshold = None\n",
        "    # Test each attribute (note attributes maybe be chosen more than once)\n",
        "    for attr in attributes:\n",
        "        df_ig = info_gain(df, attr, predict_attr, treatment_attr,\n",
        "                          method, min_bucket_t0, min_bucket_t1)\n",
        "        if df_ig is None:\n",
        "            continue\n",
        "\n",
        "        # Get the possible indices of maximum info gain\n",
        "        ig = max(df_ig['info_gain'])\n",
        "        idx_ig = df_ig.index[df_ig['info_gain']==ig]\n",
        "        # Break ties randomly\n",
        "        idx_ig = random.choice(idx_ig)\n",
        "        # Get information gain & threshold of that\n",
        "        thres = df_ig['thres'][idx_ig]\n",
        "\n",
        "        if ig > max_info_gain:\n",
        "            max_info_gain = ig\n",
        "            best_attr = attr\n",
        "            threshold = thres\n",
        "    return best_attr, threshold\n",
        "\n",
        "\n",
        "def build_tree(df, cols, predict_attr='Y', treatment_attr='T',\n",
        "               method='ED', depth=1, max_depth=float('INF'),\n",
        "               min_split=2000, min_bucket_t0=None, min_bucket_t1=None,\n",
        "               mtry=None, random_seed=3126):\n",
        "    \"\"\"\n",
        "    Builds the Decision Tree based on training data, attributes to train on,\n",
        "    and a prediction attribute\n",
        "    \"\"\"\n",
        "    if depth == 1:\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    if mtry is None:\n",
        "        mtry = math.floor(math.sqrt(len(cols)))\n",
        "    if min_bucket_t0 is None:\n",
        "        min_bucket_t0 = round(min_split/4)\n",
        "    if min_bucket_t1 is None:\n",
        "        min_bucket_t1 = round(min_split/4)\n",
        "    \n",
        "    # Get the number of positive and negative examples in the training data\n",
        "    tr, tn, cr, cn = num_class(df, predict_attr, treatment_attr)\n",
        "    r_y1_ct1 = tr / (tr + tn)\n",
        "    r_y1_ct0 = cr / (cr + cn)\n",
        "\n",
        "    # Check varialbes have less than 2 levels at the current node\n",
        "    # If not, exclude them as candidates for mtry selection\n",
        "    # To split the node, sum(ok_vars) should be equal or larger than self.mtry\n",
        "    ok_vars = []\n",
        "    for col in cols:\n",
        "        ok_vars.append(len(set(df[col])) > 1)\n",
        "\n",
        "    # Whether we have to split this node\n",
        "    #   1. min split condition: Both the sizes of treatment and control group \n",
        "    #     of an internal node should be larger than 'min_split'\n",
        "    #   2. max depth condition: The depth of tree is 'max_depth'\n",
        "    #   3. min_bucket condition: The number of treatment/control group of a\n",
        "    #     node should be larger than 'min_bucket_t0'/'min_bucket_t1'\n",
        "    #   4. Expected return should be larger than 0 and smaller than 1\n",
        "    #     (for KL-divergence & Chisq splitting criteria)\n",
        "    split_cond = tr + tn > min_split and cr + cn > min_split \\\n",
        "            and 0 < r_y1_ct1 < 1 and 0 < r_y1_ct0 < 1 \\\n",
        "            and depth < max_depth and sum(ok_vars) >= mtry\n",
        "    \n",
        "    best_attr, threshold = None, None\n",
        "    if split_cond:\n",
        "        # Sample columns\n",
        "        ok_cols = [col for col in cols if len(set(df[col])) > 1]\n",
        "        ok_cols = np.random.choice(ok_cols, mtry, replace=False)\n",
        "        # Determine attribute and its threshold value with the highest\n",
        "        # information gain\n",
        "        best_attr, threshold = choose_attr(df, ok_cols, predict_attr, treatment_attr,\n",
        "                                           method, min_bucket_t0, min_bucket_t1)\n",
        "    if best_attr is None:\n",
        "        # Create a leaf node indicating it's prediction\n",
        "        leaf = Node(None,None)\n",
        "        leaf.leaf = True\n",
        "        leaf.predict = (tr / (tr + tn), cr / (cr + cn))\n",
        "        return leaf\n",
        "    else:\n",
        "        # Create internal tree node based on attribute and it's threshold\n",
        "        sub_1 = df[df[best_attr] <= threshold]\n",
        "        sub_2 = df[df[best_attr] > threshold]\n",
        "        sub1_tr, sub1_tn, sub1_cr, sub1_cn = num_class(sub_1, predict_attr, treatment_attr)\n",
        "        sub2_tr, sub2_tn, sub2_cr, sub2_cn = num_class(sub_2, predict_attr, treatment_attr)\n",
        "        tree = Node(best_attr, threshold)\n",
        "        # Recursively build left and right subtree\n",
        "        tree.left = build_tree(sub_1, cols, predict_attr, treatment_attr,\n",
        "                               method=method, depth=depth+1, max_depth=max_depth,\n",
        "                               min_split=min_split, min_bucket_t0=min_bucket_t0, \n",
        "                               min_bucket_t1=min_bucket_t1, mtry=mtry)\n",
        "        tree.right = build_tree(sub_2, cols, predict_attr, treatment_attr,\n",
        "                               method=method, depth=depth+1, max_depth=max_depth,\n",
        "                               min_split=min_split, min_bucket_t0=min_bucket_t0, \n",
        "                               min_bucket_t1=min_bucket_t1, mtry=mtry)\n",
        "        return tree\n",
        "\n",
        "\n",
        "def predict(node, row_df):\n",
        "    \"\"\"\n",
        "    Given a instance of a training data, make a prediction of an observation (row)\n",
        "    based on the Decision Tree\n",
        "    Assumes all data has been cleaned (i.e. no NULL data)\n",
        "    \"\"\"\n",
        "    # If we are at a leaf node, return the prediction of the leaf node\n",
        "    if node.leaf:\n",
        "        return node.predict\n",
        "    # Traverse left or right subtree based on instance's data\n",
        "    if row_df[node.attr] <= node.thres:\n",
        "        return predict(node.left, row_df)\n",
        "    elif row_df[node.attr] > node.thres:\n",
        "        return predict(node.right, row_df)\n",
        "\n",
        "\n",
        "def test_predictions(root, df):\n",
        "    \"\"\"\n",
        "    Given a set of data, make a prediction for each instance using the Decision Tree\n",
        "    \"\"\"\n",
        "    pred_treat = []\n",
        "    pred_control = []\n",
        "    for index,row in df.iterrows():\n",
        "        return_treated, return_control = predict(root, row)\n",
        "        pred_treat.append(return_treated)\n",
        "        pred_control.append(return_control)\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZZouB440A56W",
        "colab": {}
      },
      "source": [
        "def eucli_dist(small_df,\n",
        "               pr_y1_ct1,\n",
        "               pr_y1_ct0,\n",
        "               pr_l,\n",
        "               pr_r,\n",
        "               pr_y1_l_ct1,\n",
        "               pr_y1_l_ct0,\n",
        "               pr_y1_r_ct1,\n",
        "               pr_y1_r_ct0,\n",
        "               pr_ct1,\n",
        "               pr_ct0,\n",
        "               pr_l_ct1,\n",
        "               pr_l_ct0):\n",
        "    # Euclidean gain\n",
        "    ed_node = (pr_y1_ct1 - pr_y1_ct0) ** 2 + ((1 - pr_y1_ct1) - (1 - pr_y1_ct0)) ** 2\n",
        "    ed_l = (pr_y1_l_ct1 - pr_y1_l_ct0) ** 2 + ((1 - pr_y1_l_ct1) - (1 - pr_y1_l_ct0)) ** 2\n",
        "    ed_r = (pr_y1_r_ct1 - pr_y1_r_ct0) ** 2 + ((1 - pr_y1_r_ct1) - (1 - pr_y1_r_ct0)) ** 2\n",
        "    ed_lr = pr_l * ed_l + pr_r * ed_r\n",
        "    ed_gain = ed_lr - ed_node\n",
        "\n",
        "    # Euclidean Normalization factor\n",
        "    gini_ct = 2 * pr_ct1 * (1 - pr_ct1)\n",
        "    ed_ct = (pr_l_ct1 - pr_l_ct0) ** 2 + ((1 - pr_l_ct1) - (1 - pr_l_ct0)) ** 2\n",
        "    gini_ct1 = 2 * pr_l_ct1 * (1 - pr_l_ct1)\n",
        "    gini_ct0 = 2 * pr_l_ct0 * (1 - pr_l_ct0)\n",
        "    ed_norm = gini_ct * ed_ct + gini_ct1 * pr_ct1 + gini_ct0 * pr_ct0 + 0.5\n",
        "    \n",
        "    # Output\n",
        "    info_gain_t = ed_gain / ed_norm\n",
        "\n",
        "    return info_gain_t\n",
        "\n",
        "\n",
        "def kl_divergence(small_df,\n",
        "                  pr_y1_ct1,\n",
        "                  pr_y1_ct0,\n",
        "                  pr_l,\n",
        "                  pr_r,\n",
        "                  pr_y1_l_ct1,\n",
        "                  pr_y1_l_ct0,\n",
        "                  pr_y1_r_ct1,\n",
        "                  pr_y1_r_ct0,\n",
        "                  pr_ct1,\n",
        "                  pr_ct0,\n",
        "                  pr_l_ct1,\n",
        "                  pr_l_ct0):\n",
        "    # KL Gain\n",
        "    kl_node = pr_y1_ct1 * np.log2(pr_y1_ct1/pr_y1_ct0) + \\\n",
        "             (1 - pr_y1_ct1) * np.log2((1 - pr_y1_ct1) / (1 - pr_y1_ct0))\n",
        "    kl_l = pr_y1_l_ct1 * np.log2(pr_y1_l_ct1 / pr_y1_l_ct0) + \\\n",
        "          (1 - pr_y1_l_ct1) * np.log2((1 - pr_y1_l_ct1) / (1 - pr_y1_l_ct0))\n",
        "    kl_r = pr_y1_r_ct1 * np.log2(pr_y1_r_ct1 / pr_y1_r_ct0) + \\\n",
        "          (1 - pr_y1_r_ct1) * np.log2((1 - pr_y1_r_ct1) / (1 - pr_y1_r_ct0))\n",
        "    kl_lr = pr_l * kl_l + pr_r * kl_r\n",
        "    kl_gain = kl_lr - kl_node\n",
        "\n",
        "    # KL Normalization factor\n",
        "    ent_ct = -(pr_ct1 * np.log2(pr_ct1) + pr_ct0 * np.log2(pr_ct0))\n",
        "    kl_ct = pr_l_ct1 * np.log2(pr_l_ct1 / pr_l_ct0) + \\\n",
        "           (1 - pr_l_ct1) * np.log2 ((1 - pr_l_ct1) / (1 - pr_l_ct0))\n",
        "    ent_ct1 = -(pr_l_ct1 * np.log2(pr_l_ct1) + (1 - pr_l_ct1) * np.log2((1 - pr_l_ct1)))\n",
        "    ent_ct0 = -(pr_l_ct0 * np.log2(pr_l_ct0) + (1 - pr_l_ct0) * np.log2((1 - pr_l_ct0)))\n",
        "\n",
        "    norm = kl_ct * ent_ct + ent_ct1 * pr_ct1 + ent_ct0 * pr_ct0 + 0.5\n",
        "\n",
        "    # Output\n",
        "    info_gain_t = kl_gain / norm\n",
        "\n",
        "    return info_gain_t\n",
        "\n",
        "\n",
        "def chisq(small_df,\n",
        "          pr_y1_ct1,\n",
        "          pr_y1_ct0,\n",
        "          pr_l,\n",
        "          pr_r,\n",
        "          pr_y1_l_ct1,\n",
        "          pr_y1_l_ct0,\n",
        "          pr_y1_r_ct1,\n",
        "          pr_y1_r_ct0,\n",
        "          pr_ct1,\n",
        "          pr_ct0,\n",
        "          pr_l_ct1,\n",
        "          pr_l_ct0):\n",
        "    # Chi-squared gain\n",
        "    chisq_node = ((pr_y1_ct1 - pr_y1_ct0) ** 2) / pr_y1_ct0 + \\\n",
        "                (((1 - pr_y1_ct1) - (1 - pr_y1_ct0)) ** 2) / (1 - pr_y1_ct0) \n",
        "    chisq_l = ((pr_y1_l_ct1 - pr_y1_l_ct0) ** 2) / pr_y1_l_ct0 + \\\n",
        "             (((1 - pr_y1_l_ct1) - (1 - pr_y1_l_ct0)) ** 2) / (1 - pr_y1_l_ct0)\n",
        "    chisq_r = ((pr_y1_r_ct1 - pr_y1_r_ct0) ** 2) / pr_y1_r_ct0 + \\\n",
        "             (((1 - pr_y1_r_ct1) - (1 - pr_y1_r_ct0)) ** 2) / (1 - pr_y1_r_ct0)\n",
        "    chisq_lr = pr_l * chisq_l + pr_r * chisq_r\n",
        "    chisq_gain = chisq_lr - chisq_node\n",
        "\n",
        "    # Chi-squared Normalization factor\n",
        "    gini_ct = 2 * pr_ct1 * (1 - pr_ct1) \n",
        "    chisq_ct = ((pr_l_ct1 - pr_l_ct0) ** 2) / pr_l_ct0 + \\\n",
        "              (((1 - pr_l_ct1) - (1 - pr_l_ct0)) ** 2) / (1 - pr_l_ct0)\n",
        "    gini_ct1 = 2 * pr_l_ct1 * (1 - pr_l_ct1)\n",
        "    gini_ct0 = 2 * pr_l_ct0 * (1 - pr_l_ct0)\n",
        "    chisq_norm = gini_ct * chisq_ct + gini_ct1 * pr_ct1  + gini_ct0 * pr_ct0 + 0.5\n",
        "     \n",
        "    # Output\n",
        "    info_gain_t = chisq_gain / chisq_norm\n",
        "\n",
        "    return info_gain_t\n",
        "\n",
        "\n",
        "def interaction_split(small_df,\n",
        "                      pr_y1_ct1,\n",
        "                      pr_y1_ct0,\n",
        "                      pr_l,\n",
        "                      pr_r,\n",
        "                      pr_y1_l_ct1,\n",
        "                      pr_y1_l_ct0,\n",
        "                      pr_y1_r_ct1,\n",
        "                      pr_y1_r_ct0,\n",
        "                      pr_ct1,\n",
        "                      pr_ct0,\n",
        "                      pr_l_ct1,\n",
        "                      pr_l_ct0,\n",
        "                      cs_ct1,\n",
        "                      cs_ct0,\n",
        "                      ncs_ct1,\n",
        "                      ncs_ct0):\n",
        "    # Compute elements for split formula\n",
        "    C44 = 1/cs_ct1 + 1/cs_ct0 + 1/ncs_ct1 + 1/ncs_ct0\n",
        "\n",
        "    UR = pr_y1_r_ct1 - pr_y1_r_ct0\n",
        "    UL = pr_y1_l_ct1 - pr_y1_l_ct0\n",
        "\n",
        "    SSE = cs_ct1 * pr_y1_l_ct1 * (1 - pr_y1_l_ct1) + \\\n",
        "         ncs_ct1 * pr_y1_r_ct1 * (1 - pr_y1_r_ct1) + \\\n",
        "         cs_ct0 * pr_y1_l_ct0 * (1 - pr_y1_l_ct0)  + \\\n",
        "         ncs_ct0 * pr_y1_r_ct0 * (1 - pr_y1_r_ct0)\n",
        "         \n",
        "    n_node = len(small_df)       \n",
        "\n",
        "    # Output: Interaction split\n",
        "    info_gain_t = ((n_node - 4) * (UR - UL)**2) / (C44 * SSE)\n",
        "\n",
        "    return info_gain_t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ORX-_PRzfNw",
        "colab": {}
      },
      "source": [
        "def uplift_tree(x, y, t, **kwargs):\n",
        "    predict_attr = kwargs.get('predict_attr', 'Y')\n",
        "    treatment_attr = kwargs.get('treatment_attr', 'T')\n",
        "    \n",
        "    df = x.copy()\n",
        "    df[predict_attr] = y\n",
        "    df[treatment_attr] = t\n",
        "    \n",
        "    kwargs['predict_attr'] = predict_attr\n",
        "    kwargs['treatment_attr'] = treatment_attr\n",
        "    root = build_tree(df, x.columns, **kwargs)\n",
        "    \n",
        "    return root\n",
        "\n",
        "\n",
        "def predict_tree(root, newdata, **kwargs):\n",
        "    return test_predictions(root, newdata)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EC1PVHpyg7jG",
        "colab": {}
      },
      "source": [
        "def upliftRF(x, y, t, ntree=10, bagging_fraction=0.6, random_seed=3126, **kwargs):\n",
        "    predict_attr = kwargs.get('predict_attr', 'Y')\n",
        "    treatment_attr = kwargs.get('treatment_attr', 'T')\n",
        "    \n",
        "    df = x.copy()\n",
        "    df[predict_attr] = y\n",
        "    df[treatment_attr] = t\n",
        "    \n",
        "    kwargs['predict_attr'] = predict_attr\n",
        "    kwargs['treatment_attr'] = treatment_attr\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "    random_seeds = [np.random.randint(10000) for _ in range(ntree)]\n",
        "    trees = []\n",
        "    for i in range(ntree):\n",
        "        bagged_df = df.sample(frac=bagging_fraction, random_state=random_seeds[i])\n",
        "        trees.append(build_tree(bagged_df, x.columns, random_seed=random_seeds[i], **kwargs))\n",
        "    \n",
        "    return trees\n",
        "\n",
        "\n",
        "def predict_upliftRF(obj, newdata, **kwargs):\n",
        "    pred_trees = []\n",
        "    for tree in obj:\n",
        "        pred_trees.append(test_predictions(tree, newdata))\n",
        "\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": sum([x['pr_y1_t1'] for x in pred_trees])/len(pred_trees),\n",
        "        \"pr_y1_t0\": sum([x['pr_y1_t0'] for x in pred_trees])/len(pred_trees),\n",
        "    })\n",
        "    return pred_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uj3olZCN_-ML",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('Hillstrom.csv')\n",
        "dataset = 'hillstrom'\n",
        "# df = pd.read_csv('criteo_small_fix.csv')\n",
        "# dataset = 'criteo'\n",
        "df = preprocess_data(df, dataset=dataset)\n",
        "Y = df['Y']\n",
        "T = df['T']\n",
        "X = df.drop(['Y', 'T'], axis=1)\n",
        "ty = pd.DataFrame({'Y': Y, 'T': T})\\\n",
        "         .apply(lambda row: ty_assign(row['Y'], row['T']), axis=1)\n",
        "\n",
        "\n",
        "### Experiment procedure ###\n",
        "# search_space = {\n",
        "#     'ntree': [10, ],\n",
        "#     'mtry': [3, ],\n",
        "#     'bagging_fraction': [0.6, ],\n",
        "#     'method': ['ED',],\n",
        "#     'max_depth': [10, ],\n",
        "#     'min_split': [1000, ],\n",
        "#     'min_bucket_t0': [100,],\n",
        "#     'min_bucket_t1': [100,],\n",
        "# }\n",
        "# method = upliftRF\n",
        "# predict_method = predict_upliftRF\n",
        "\n",
        "search_space = {\n",
        "    'solver': ['liblinear',],\n",
        "}\n",
        "# method = tma\n",
        "# predict_method = predict_tma\n",
        "\n",
        "# search_space = {\n",
        "#     'learning_rate': [0.1,],\n",
        "# }\n",
        "method = dta\n",
        "predict_method = predict_dta\n",
        "\n",
        "q_list = []\n",
        "fold_gen = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234).split(X, ty)\n",
        "for idx, (train_index, test_index) in enumerate(fold_gen):\n",
        "    X_train = X.reindex(train_index)\n",
        "    X_test = X.reindex(test_index)\n",
        "    Y_train = Y.reindex(train_index)\n",
        "    Y_test = Y.reindex(test_index)\n",
        "    T_train = T.reindex(train_index)\n",
        "    T_test = T.reindex(test_index)\n",
        "\n",
        "    df = X_train.copy()\n",
        "    df['Y'] = Y_train\n",
        "    df['T'] = T_train\n",
        "    stratify = T_train\n",
        "    if dataset == 'hillstrom':\n",
        "        stratify = df[['Y', 'T']]\n",
        "    tuning_df, validate_df = train_test_split(\n",
        "        df, test_size=0.33, random_state=1234, stratify=stratify)\n",
        "\n",
        "    X_tuning = tuning_df.drop(['Y', 'T'], axis=1)\n",
        "    Y_tuning = tuning_df['Y']\n",
        "    T_tuning = tuning_df['T']\n",
        "    X_validate = validate_df.drop(['Y', 'T'], axis=1)\n",
        "    Y_validate = validate_df['Y']\n",
        "    T_validate = validate_df['T']\n",
        "    data_dict = {\n",
        "        \"x_train\": X_tuning,\n",
        "        \"y_train\": Y_tuning,\n",
        "        \"t_train\": T_tuning,\n",
        "        \"x_test\": X_validate,\n",
        "        \"y_test\": Y_validate,\n",
        "        \"t_test\": T_validate,\n",
        "    }\n",
        "\n",
        "    _, best_params = parameter_tuning(method, predict_method, data_dict, \n",
        "                                      search_space=search_space, plotit=False)\n",
        "    best_params = {k:v[0] for k, v in search_space.items()}\n",
        "    trained_model = method(X_train, Y_train, T_train, **best_params)\n",
        "    pred = predict_method(trained_model, X_test, y=Y_test, t=T_test)\n",
        "    perf = performance(pred['pr_y1_t1'], pred['pr_y1_t0'], Y_test, T_test)\n",
        "    q = qini(perf)\n",
        "    q_list.append(q)\n",
        "    print(\"Best_params: \", best_params)\n",
        "print(\"Method: {}\".format(method))\n",
        "print(\"search space:\", search_space)\n",
        "qini_list = [q['qini'] for q in q_list]\n",
        "print('Qini values: ', qini_list)\n",
        "print('    mean: {}, std: {}'.format(np.mean(qini_list), np.std(qini_list)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09tJ63FTtwfp",
        "colab": {}
      },
      "source": [
        "trained_model.classes_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aOPp6WQ7rFNF",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def z_assign(y, t):\n",
        "    \"\"\" Define transformed response variable z\n",
        "    if (treated and response) or (not treated and not response), return 1\n",
        "    else, return 0\n",
        "    \"\"\"\n",
        "    if y == 1 and t == 1:\n",
        "        return 1\n",
        "    elif y == 0 and t == 1:\n",
        "        return 0\n",
        "    elif y == 1 and t == 0:\n",
        "        return 0\n",
        "    elif y == 0 and t == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def lai(x, y, t, method=GradientBoostingClassifier, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Lai's Approach\" \n",
        "    The default model is Gradient Boosting Machine (gbm)\n",
        "\n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        A sklearn model.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({'y': y.copy()})\n",
        "    df['t'] = t\n",
        "    z = df.apply(lambda row: z_assign(row['y'], row['t']), axis=1)\n",
        "    \n",
        "    model = method(**kwargs).fit(x, z)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_lai(obj, newdata, y, t, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Lai's Approach\" \n",
        "    \n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        obj: A sklearn model.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        dataframe: A dataframe with predictions for when the instances are\n",
        "            treated and for when they are not treated.\n",
        "    \"\"\"\n",
        " \n",
        "    pred = obj.predict_proba(newdata)    # list of [False, True]\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"pr_y1_t1\": [row[1] for row in pred],\n",
        "        \"pr_y1_t0\": [row[0] for row in pred],\n",
        "    })\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c0psF5SuwgWP",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def glai(x, y, t, method=GradientBoostingClassifier, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Lai's Approach\" \n",
        "    The default model is Gradient Boosting Machine (gbm)\n",
        "\n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        A sklearn model.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({'y': y.copy()})\n",
        "    df['t'] = t\n",
        "    ty = df.apply(lambda row: ty_assign(row['y'], row['t']), axis=1)\n",
        "    \n",
        "    model = method(**kwargs).fit(x, ty)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_glai(obj, newdata, y, t, **kwargs):\n",
        "    \"\"\"Predictions according to the \"Lai's Approach\" \n",
        "    \n",
        "    Source: \"Influential Marketing\" (Lai, 2006) and \"Mining Truly Responsive\n",
        "            Customers Using True Lift Overview\" (Kane, 2014)\n",
        "\n",
        "    Args:\n",
        "        obj: A sklearn model.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        dataframe: A dataframe with predictions for when the instances are\n",
        "            treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "    prob_T = sum(t) / len(t)\n",
        "    prob_C = 1 - prob_T\n",
        "    \n",
        "    pred = obj.predict_proba(newdata)    # list of [CN, CR, TN, TR]\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"pr_y1_t1\": [row[3]/prob_T + row[0]/prob_C for row in pred],   # TR/T + CN/C\n",
        "        \"pr_y1_t0\": [row[2]/prob_T + row[1]/prob_C for row in pred],   # TN/T + CR/C\n",
        "    })\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lqztMu4voFLV",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def z_assign(y, t):\n",
        "    \"\"\" Define transformed response variable z\n",
        "    if (treated and response) or (not treated and not response), return 1\n",
        "    else, return 0\n",
        "    \"\"\"\n",
        "    if y == 1 and t == 1:\n",
        "        return 1\n",
        "    elif y == 0 and t == 1:\n",
        "        return 0\n",
        "    elif y == 1 and t == 0:\n",
        "        return 0\n",
        "    elif y == 0 and t == 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rvtu(x, y, t, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Transforming the data according to the \"Jaskowski's Approach\"\n",
        "    Sometimes, it called Response Variable Transformation for Uplift (RVTU)\n",
        "\n",
        "    Source: \"Uplift modeling for clinical trial data\" (Jaskowski, 2006)\n",
        "    \"\"\"\n",
        "\n",
        "    ### Combine x, y, and ct\n",
        "    df = x.copy()\n",
        "    df['y'] = y\n",
        "    df['ct'] = t\n",
        "    df['z'] = df.apply(lambda row: z_assign(row['y'], row['ct']), axis=1)\n",
        "    mdl = method(**kwargs).fit(x, df['z'])\n",
        "    \n",
        "    return mdl\n",
        "\n",
        "\n",
        "def predict_rvtu(obj, newdata, y, t, **kwargs):\n",
        "    # df = pd.DataFrame({'y': y.copy()})\n",
        "    # df['ct'] = ct\n",
        "    # z = df.apply(lambda row: z_assign(row['y'], row['ct']), axis=1)\n",
        "\n",
        "    if isinstance(obj, LinearRegression):\n",
        "        pred = obj.predict(newdata)\n",
        "    else:\n",
        "        pred = obj.predict_proba(newdata)[:, 1]\n",
        "\n",
        "    res = pd.DataFrame({\n",
        "        \"pr_y1_t1\": [row for row in pred],\n",
        "        \"pr_y1_t0\": [1-row for row in pred],\n",
        "    })\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vK0a6vs9ss3R",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def dta(x, y, t, method=LogisticRegression, **kwargs):\n",
        "    \"\"\"Training a model according to the \"Dummy Treatment Approach\" \n",
        "    The default model is General Linear Model (GLM)\n",
        "\n",
        "    Source: \"The True Lift Model\" (Lo, 2002)\n",
        "\n",
        "    Args:\n",
        "        x: A data frame of predictors.\n",
        "        y: A binary response (numeric) vector.\n",
        "        t: A binary response (numeric) representing the treatment assignment\n",
        "            (coded as 0/1).\n",
        "        method: A sklearn model specifying which classification or regression\n",
        "            model to use. This should be a method that can handle a \n",
        "            multinominal class variable.\n",
        "\n",
        "    Return:\n",
        "        A sklearn model.\n",
        "    \"\"\"\n",
        "    # Create interaction variables\n",
        "    # Building our dataframe with the interaction variables\n",
        "    df = x.copy()\n",
        "    for colname in x.columns:\n",
        "        df[\"Int_\" + colname] = x[colname] * t\n",
        "    df['treated'] = t\n",
        "\n",
        "    # Fit a model\n",
        "    model = method(**kwargs).fit(df, y)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_dta(obj, newdata, y_name='y', t_name='treated', **kwargs):\n",
        "    \"\"\"Predictions according to the \"Dummy Treatment Approach\" \n",
        "    \n",
        "    For each instance in newdata two predictions are made:\n",
        "    1) What is the probability of a person responding when treated?\n",
        "    2) What is the probability of a person responding when not treated\n",
        "      (i.e. part of control group)?\n",
        "\n",
        "    Source: \"The True Lift Model\" (Lo, 2002)\n",
        "\n",
        "    Args:\n",
        "        obj: A sklearn model.\n",
        "        newdata: A data frame containing the values at which predictions\n",
        "            are required.\n",
        "    \n",
        "    Return:\n",
        "        dataframe: A dataframe with predictions for when the instances are\n",
        "            treated and for when they are not treated.\n",
        "    \"\"\"\n",
        "    predictors = [c for c in newdata.columns if c not in (y_name, t_name)]\n",
        "\n",
        "    df_treat = newdata.copy()\n",
        "    df_control = newdata.copy()\n",
        "    for colname in predictors:\n",
        "        df_treat[\"Int_\" + colname] = df_treat[colname] * 1\n",
        "        df_control[\"Int_\" + colname] = df_control[colname] * 0\n",
        "    df_treat['treated'] = 1\n",
        "    df_control['treated'] = 0\n",
        "\n",
        "    # print(obj.coef_, obj.intercept_)\n",
        "    if isinstance(obj, LinearRegression):\n",
        "        pred_treat = obj.predict(df_treat)\n",
        "        pred_control = obj.predict(df_control)\n",
        "    else:\n",
        "#         pred = obj.predict_proba(newdata)[:, 1]\n",
        "        pred_treat = obj.predict_proba(df_treat)[:, 1]\n",
        "        pred_control = obj.predict_proba(df_control)[:, 1]\n",
        "\n",
        "    pred_df = pd.DataFrame({\n",
        "        \"pr_y1_t1\": pred_treat,\n",
        "        \"pr_y1_t0\": pred_control,\n",
        "    })\n",
        "    return pred_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ExU6BW9etUpx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}